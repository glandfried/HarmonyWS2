---
title: Session 2
subtitle: Basic Hui-Walter models
date: "2021-06-28"
author:
  - Matt Denwood
theme: metropolis
aspectratio: 43
colortheme: seahorse
header-includes: 
  - \input{../rsc/preamble}
params:
  presentation: TRUE
output:
  beamer_presentation:
      pandoc_args: ["-t", "beamer"]
      slide_level: 2
  html_document: default
---

```{r rendering, eval=FALSE, include=FALSE}
# To render this as PDF (beamer) slides run:
rmarkdown::render('Session_2.Rmd', 'beamer_presentation', params=list(presentation=TRUE))
# And for html:
rmarkdown::render('Session_2.Rmd', 'html_document', params=list(presentation=FALSE))
```

```{r setup, include=FALSE}
library("tidyverse")
set.seed(2021-06-22)

# Reduce the width of R code output for PDF only:
if(params$presentation) options(width=60)
knitr::opts_chunk$set(echo = TRUE)

# Reduce font size of R code output for Beamer:
if(params$presentation){
  knitr::knit_hooks$set(size = function(before, options, envir) {
    if(before){
      knitr::asis_output(paste0("\\", options$size))
    }else{
      knitr::asis_output("\\normalsize")
    }
  })
  knitr::opts_chunk$set(size = "scriptsize")
}

# Collapse successive chunks:
space_collapse <- function(x){ gsub("```\n*```r*\n*", "", x) }
# Reduce space between chunks:
space_reduce <- function(x){ gsub("```\n+```\n", "", x) }
knitr::knit_hooks$set(document = space_collapse)
```

# Session 2:  Basic Hui-Walter models

## Hui-Walter Model

- A particular model formulation that was originally designed for evaluating diagnostic tests in the absence of a gold standard

- Not necessarily (or originally) Bayesian but often implemented using Bayesian MCMC
  
- But evaluating an imperfect test against another imperfect test is a bit like pulling a rabbit out of a hat
  * If we don't know the true disease status, how can we estimate sensitivity or specificity for either test?


## Model Specification


```{r include=FALSE}
hw_definition <- c("model{
  Tally ~ dmulti(prob, N)
  
  # Test1- Test2-
	prob[1] <- (prev * ((1-se[1])*(1-se[2]))) + ((1-prev) * ((sp[1])*(sp[2])))

  # Test1+ Test2-
	prob[2] <- (prev * ((se[1])*(1-se[2]))) + ((1-prev) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob[3] <- (prev * ((1-se[1])*(se[2]))) + ((1-prev) * ((sp[1])*(1-sp[2])))
", " 
  # Test1+ Test2+
	prob[4] <- (prev * ((se[1])*(se[2]))) + ((1-prev) * ((1-sp[1])*(1-sp[2])))

  prev ~ dbeta(1, 1)
  se[1] ~ dbeta(1, 1)
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)
  sp[2] ~ dbeta(1, 1)

  #data# Tally, N
  #monitor# prev, prob, se, sp, deviance
  #inits# prev, se, sp
}
")
cat(hw_definition, sep='', file='basic_hw.bug')
```


```{r comment='', echo=FALSE}
cat(hw_definition[1], sep='\n')
```

---

```{r comment='', echo=FALSE}
cat(hw_definition[2], sep='\n')
```

---

```{r}
twoXtwo <- matrix(c(48, 12, 4, 36), ncol=2, nrow=2)
twoXtwo
```


```{r, message=FALSE, warning=FALSE, results='hide'}
library('runjags')

Tally <- as.numeric(twoXtwo)
N <- sum(Tally)

prev <- list(chain1=0.05, chain2=0.95)
se <- list(chain1=c(0.01,0.99), chain2=c(0.99,0.01))
sp <- list(chain1=c(0.01,0.99), chain2=c(0.99,0.01))

results <- run.jags('basic_hw.bug', n.chains=2)
```

[Remember to check convergence and effective sample size!]

---

```{r, eval=FALSE}
results
```

```{r echo=FALSE, results="hide"}
inits1 <- list(.RNG.name="base::Super-Duper", .RNG.seed=8)
inits2 <- list(.RNG.name="base::Wichmann-Hill", .RNG.seed=9)

results <- run.jags('basic_hw.bug', n.chains=2, inits=list(inits1,inits2), silent.jags=TRUE)
results

pt <- plot(results)
```

```{r echo=FALSE}
res <- summary(results)[,c(1:3,9,11)]
res[] <- round(res, 3)
knitr::kable(res)
```

- Does anybody spot a problem?

- - -

```{r echo=FALSE}
print(pt[["prob[1].plot1"]])
```

- - -

```{r echo=FALSE}
print(pt[["prev.plot1"]])
```

- - -

```{r echo=FALSE}
print(pt[["se[1].plot1"]])
```

- - -

```{r echo=FALSE}
print(pt[["sp[1].plot1"]])
```

- - -

TODO:  add deviance plot

- - -

```{r echo=FALSE}
print(pt[["crosscorr"]])
```

## Label Switching

How to interpret a test with Se=0% and Sp=0%?

. . .

  * The test is perfect - we are just holding it upside down...

. . .

We can force se+sp >= 1:

```{r eval=FALSE}
  se[1] ~ dbeta(1, 1)
  sp[1] ~ dbeta(1, 1)T(1-se[1], )
```

Or:

```{r eval=FALSE}
  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
```

This allows the test to be useless, but not worse than useless.

Note: the joint posterior is not necessarily what you would expect

*TODO* show model without data

Alternative: rejection step (ability to be added to JAGS/runjags at some point)


- - -

Alternatively we can have the weakly informative priors:

```{r eval=FALSE}
  se[1] ~ dbeta(2, 1)
  sp[1] ~ dbeta(2, 1)
```

To give the model some information that we expect the test characteristics to be closer to 100% than 0%.

Or we can use stronger priors for one or both tests.

*TODO*: show results with truncated priors

## Practicalities

- Be **very** vareful with the order of combinations in dmultinom!

- Check your results carefully to ensure they make sense!

- Convergence is more problematic than usual

- These models need A LOT of data, and/or strong priors for one of the tests


## Priors

## A different prior

- A quick way to see the distribution of a prior:

```{r, fig.width=3, fig.height=3}
curve(dbeta(x, 1, 1), from=0, to=1)
qbeta(c(0.025,0.975), shape1=1, shape2=1)
```

---

- This was minimally informative, but how does that compare to a weakly informative prior for e.g. sensitivity?

```{r, fig.width=3, fig.height=3}
curve(dbeta(x, 2, 1), from=0, to=1)
qbeta(c(0.025,0.975), shape1=2, shape2=1)
```

- Or more accurately:

```{r}
library("TeachingDemos")
hpd(qbeta, shape1=2, shape2=1)
```

---

- What about a more informative prior?

```{r, fig.width=3, fig.height=3}
curve(dbeta(x, 20, 2), from=0, to=1)
hpd(qbeta, shape1=20, shape2=2)
```

## Choosing a prior

- Typically we are given median and 95% confidence intervals from a paper, e.g.:

"The median (95% CI) estimates of the sensitivity and specificity of the shiny new test were 94% (92-96%) and 99% (97-100%) respectively"

- How can we generate a prior from this?

## The PriorGen package

"The median (95% CI) estimates of the sensitivity and specificity of the shiny new test were 94% (92-96%) and 99% (97-100%) respectively"

```{r}
library("PriorGen")
findbeta(themedian = 0.94, percentile=0.95, percentile.value = 0.92)
```
```{r}
curve(dbeta(x, shape1=429.95, shape2=27.76))
hpd(qbeta, shape1=429.95, shape2=27.76)
```


## Analysing simulated data

This is useful to check that we can recover parameter values!

```{r}
se <- c(0.9, 0.6)
sp <- c(0.95, 0.9)
N <- 1000
prevalence <- 0.25

data <- tibble(Status = rbinom(N, 1, prevalence)) %>%
  mutate(Test1 = rbinom(N, 1, se[1]*Status + (1-sp[1])*(1-Status))) %>%
  mutate(Test2 = rbinom(N, 1, se[2]*Status + (1-sp[2])*(1-Status)))

twoXtwo <- with(data, table(Test1, Test2))
Tally <- as.numeric(twoXtwo)
```


# Practical Session 2

## Points to consider {.fragile}

1. What is the typical autocorrelation (and therefore effective sample size) of Hui-Walter models compared to the simpler models we were running earlier?  Is there any practical consequence of this?

1. How does changing the prior distributions for the se and sp of one test affect the inference for the other test parameters?

## Priors

- We cannot estimate `se`, `sp` and `prevalence` simultaneously
  
  * We need strong priors for se and sp

- We can use the PriorGen package to generate Beta priors based on published results, for example:

```{r}
PriorGen::findbeta(themean=0.9, percentile = 0.975, percentile.value = 0.8)
```

---

```{r}
qbeta(c(0.025, 0.5, 0.975), 41.82, 4.65)
curve(dbeta(x, 41.82, 4.65), from=0, to=1)
```


## Exercise {.fragile}

- Find beta distribution priors for:

  * Sensitivity = 0.9 (95% CI: 0.85 - 0.95)
  * Specificity = 0.95 (95%CI: 0.92-0.97)

- Look at these distributions using curve and qbeta

- Modify the imperfect test model using these priors and re-estimate prevalence


`r if(params$presentation) {"\\begin{comment}"}`

## Solution {.fragile}

Parameters for Sensitivity = 0.9 (95% CI: 0.85 - 0.95):


```{r}
PriorGen::findbeta(themean=0.9, percentile = 0.975, percentile.value = 0.85)
qbeta(c(0.025, 0.5, 0.975), 148.43, 16.49)
curve(dbeta(x, 148.43, 16.49), from=0, to=1)
```

Parameters for Specificity = 0.95 (95%CI: 0.92-0.97):

```{r}
PriorGen::findbeta(themean=0.95, percentile = 0.975, percentile.value = 0.92)
qbeta(c(0.025, 0.5, 0.975), 240.03, 12.63)
curve(dbeta(x, 240.03, 12.63), from=0, to=1)
```

Updated model:


```{r include=FALSE}
imperfect_definition <- "model{
  Positives ~ dbinom(obsprev, TotalTests)
  obsprev <- (prevalence * se) + ((1-prevalence) * (1-sp))
  
  prevalence ~ dbeta(1, 1)
  se ~ dbeta(148.43, 16.49)
  sp ~ dbeta(240.03, 12.63)
  
  #data# Positives, TotalTests
  #monitor# prevalence, obsprev, se, sp
  #inits# prevalence, se, sp
}
"
cat(imperfect_definition, file='basicimperfect.bug')
```

```{r comment='', echo=FALSE}
cat(imperfect_definition, sep='\n')
```

Results:

```{r}
prevalence <- list(chain1=0.05, chain2=0.95)
se <- list(chain1=0.5, chain2=0.99)
sp <- list(chain1=0.5, chain2=0.99)
Positives <- 70
TotalTests <- 100
results <- run.jags('basicimperfect.bug', n.chains=2, burnin=0, sample=10000)
results
```

This time we get sensible estimates for `prevalence`.

`r if(params$presentation) {"\\end{comment}"}`

## Optional Exercise {.fragile}

- Run the same model with se and sp fixed to the mean estimate

  * How does this affect CI for prevalence?

- Run the same model with se and sp fixed to 1

  * How does this affect estimates and CI for prevalence?


`r if(params$presentation) {"\\begin{comment}"}`

## Optional Solution {.fragile}

Same model with se and sp fixed to the mean estimates:

```{r include=FALSE}
imperfect_definition <- "model{
  Positives ~ dbinom(obsprev, TotalTests)
  obsprev <- (prevalence * se) + ((1-prevalence) * (1-sp))
  
  prevalence ~ dbeta(1, 1)
  se <- 0.9
  # se ~ dbeta(148.43, 16.49)
  sp <- 0.95
  # sp ~ dbeta(240.03, 12.63)
  
  #data# Positives, TotalTests
  #monitor# prevalence, obsprev, se, sp
  #inits# prevalence
}
"
cat(imperfect_definition, file='basicimperfect.bug')
```

```{r comment='', echo=FALSE}
cat(imperfect_definition, sep='\n')
```

Note that we have to remove se and sp from the initial values.  Results:

```{r}
results <- run.jags('basicimperfect.bug', n.chains=2, burnin=0, sample=10000)
results
```

Estimates for `prevalence` are a little bit more precise.

Same model with se and sp fixed to one:

```{r include=FALSE}
imperfect_definition <- "model{
  Positives ~ dbinom(obsprev, TotalTests)
  obsprev <- (prevalence * se) + ((1-prevalence) * (1-sp))
  
  prevalence ~ dbeta(1, 1)
  se <- 1
  # se ~ dbeta(148.43, 16.49)
  sp <- 1
  # sp ~ dbeta(240.03, 12.63)
  
  #data# Positives, TotalTests
  #monitor# prevalence, obsprev, se, sp
  #inits# prevalence
}
"
cat(imperfect_definition, file='basicimperfect.bug')
```

```{r comment='', echo=FALSE}
cat(imperfect_definition, sep='\n')
```

Results:

```{r}
results <- run.jags('basicimperfect.bug', n.chains=2, burnin=0, sample=10000)
results
```

Biased estimates for `prevalence`!

`r if(params$presentation) {"\\end{comment}"}`

`r if(params$presentation) {"\\begin{comment}"}`

## Exercise plan TODO

- Simulate data with different sample sizes and analyse it using:
  - Model with minimally informative priors for everything
  - Truncated minimally informative priors
  - Informative priors for one test (give them median and 95% CI estimates)

- Change the simulated prevalence
  - Low prevalence = Sp can be estimated
  - High prevalence = Se can be estimated

Optional:

- Explore prior - likelihood conflict
  - Priors taken from manufacturer's numbers
  - Real-world Se/Sp lower - observe bias in posterior

`r if(params$presentation) {"\\end{comment}"}`


## Summary {.fragile}

- Hui-Walter models are like `pulling a rabbit out of a hat`, but:
  - They typically exhibit high autocorrelation
  - They may not converge immediately
  - We need a larger sample for the same effective sample size
  
- More informative priors for one test will
  - Improve identifiability of the model
  - Impact the inference for all other parameters in the model!

