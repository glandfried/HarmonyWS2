---
title: Session 2
subtitle: Basic Hui-Walter models
date: "2021-06-28"
author:
  - Matt Denwood
theme: metropolis
aspectratio: 43
colortheme: seahorse
header-includes: 
  - \input{../rsc/preamble}
params:
  presentation: TRUE
output:
  beamer_presentation:
      pandoc_args: ["-t", "beamer"]
      slide_level: 2
  html_document: default
---

```{r rendering, eval=FALSE, include=FALSE}
# To render this as PDF (beamer) slides run:
rmarkdown::render('Session_2.Rmd', 'beamer_presentation', params=list(presentation=TRUE))
# And for html:
rmarkdown::render('Session_2.Rmd', 'html_document', params=list(presentation=FALSE))
```

```{r setup, include=FALSE}
library("tidyverse")
library("runjags")
runjags.options(silent.jags=TRUE, silent.runjags=TRUE)
set.seed(2021-06-22)

# Reduce the width of R code output for PDF only:
if(params$presentation) options(width=60)
knitr::opts_chunk$set(echo = TRUE)

# Reduce font size of R code output for Beamer:
if(params$presentation){
  knitr::knit_hooks$set(size = function(before, options, envir) {
    if(before){
      knitr::asis_output(paste0("\\", options$size))
    }else{
      knitr::asis_output("\\normalsize")
    }
  })
  knitr::opts_chunk$set(size = "scriptsize")
}

# Collapse successive chunks:
space_collapse <- function(x){ gsub("```\n*```r*\n*", "", x) }
# Reduce space between chunks:
space_reduce <- function(x){ gsub("```\n+```\n", "", x) }
knitr::knit_hooks$set(document = space_collapse)

# To collect temporary filenames:
cleanup <- character(0)
```

## Hui-Walter Model

- A particular model formulation that was originally designed for evaluating diagnostic tests in the absence of a gold standard

- Not necessarily (or originally) Bayesian but often implemented using Bayesian MCMC
  
- But evaluating an imperfect test against another imperfect test is a bit like pulling a rabbit out of a hat
  * If we don't know the true disease status, how can we estimate sensitivity or specificity for either test?


## Model Specification


```{r include=FALSE}
hw_definition <- c("model{
  Tally ~ dmulti(prob, N)
  
  # Test1- Test2-
	prob[1] <- (prev * ((1-se[1])*(1-se[2]))) + ((1-prev) * ((sp[1])*(sp[2])))

  # Test1+ Test2-
	prob[2] <- (prev * ((se[1])*(1-se[2]))) + ((1-prev) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob[3] <- (prev * ((1-se[1])*(se[2]))) + ((1-prev) * ((sp[1])*(1-sp[2])))
", " 
  # Test1+ Test2+
	prob[4] <- (prev * ((se[1])*(se[2]))) + ((1-prev) * ((1-sp[1])*(1-sp[2])))

  prev ~ dbeta(1, 1)
  se[1] ~ dbeta(1, 1)
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)
  sp[2] ~ dbeta(1, 1)

  #data# Tally, N
  #monitor# prev, prob, se, sp, deviance
  #inits# prev, se, sp
}
")
cat(hw_definition, sep='', file='basic_hw.txt')
```


```{r comment='', echo=FALSE}
cleanup <- c(cleanup, 'basic_hw.txt')
cat(hw_definition[1], sep='\n')
```

---

```{r comment='', echo=FALSE}
cat(hw_definition[2], sep='\n')
```

---

```{r}
twoXtwo <- matrix(c(48, 12, 4, 36), ncol=2, nrow=2)
twoXtwo
```


```{r, message=FALSE, warning=FALSE, results='hide'}
library('runjags')

Tally <- as.numeric(twoXtwo)
N <- sum(Tally)

prev <- list(chain1=0.05, chain2=0.95)
se <- list(chain1=c(0.01,0.99), chain2=c(0.99,0.01))
sp <- list(chain1=c(0.01,0.99), chain2=c(0.99,0.01))

results <- run.jags('basic_hw.txt', n.chains=2)
```

[Remember to check convergence and effective sample size!]

---

```{r, eval=FALSE}
results
```

```{r include=FALSE}
hw_definition_inits <- c("model{
  Tally ~ dmulti(prob, N)
  
  # Test1- Test2-
	prob[1] <- (prev * ((1-se[1])*(1-se[2]))) + ((1-prev) * ((sp[1])*(sp[2])))

  # Test1+ Test2-
	prob[2] <- (prev * ((se[1])*(1-se[2]))) + ((1-prev) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob[3] <- (prev * ((1-se[1])*(se[2]))) + ((1-prev) * ((sp[1])*(1-sp[2])))
", " 
  # Test1+ Test2+
	prob[4] <- (prev * ((se[1])*(se[2]))) + ((1-prev) * ((1-sp[1])*(1-sp[2])))

  prev ~ dbeta(1, 1)
  se[1] ~ dbeta(1, 1)
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)
  sp[2] ~ dbeta(1, 1)

  #data# Tally, N
  #monitor# prev, prob, se, sp, deviance
  #inits# prev, se, sp, .RNG.name, .RNG.seed
}
")
cat(hw_definition_inits, sep='', file='basic_hw_inits.txt')
cleanup <- c(cleanup, 'basic_hw_inits.txt')
```

```{r echo=FALSE, results="hide"}
.RNG.name <- list("base::Super-Duper", "base::Wichmann-Hill")
.RNG.seed <- list(15, 16)
results <- run.jags('basic_hw_inits.txt', n.chains=2, silent.jags=TRUE)
results

pt <- plot(results)
```

```{r echo=FALSE}
res <- summary(results)[,c(1:3,9,11)]
res[] <- round(res, 3)
knitr::kable(res)
```

- Does anybody spot a problem?

- - -

```{r echo=FALSE}
print(pt[["prob[1].plot1"]])
```

- - -

```{r echo=FALSE}
print(pt[["prev.plot1"]])
```

- - -

```{r echo=FALSE}
print(pt[["se[1].plot1"]])
```

- - -

```{r echo=FALSE}
print(pt[["sp[1].plot1"]])
```

- - -

```{r echo=FALSE}
print(pt[["deviance.plot1"]])
```

- - -

```{r echo=FALSE}
print(pt[["crosscorr"]])
```

## Label Switching

How to interpret a test with Se=0% and Sp=0%?

. . .

  * The test is perfect - we are just holding it upside down...

. . .

We can force se+sp >= 1:

```{r eval=FALSE}
  se[1] ~ dbeta(1, 1)
  sp[1] ~ dbeta(1, 1)T(1-se[1], )
```

Or:

```{r eval=FALSE}
  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
```

This allows the test to be useless, but not worse than useless.

- - -

Alternatively we can have the weakly informative priors:

```{r eval=FALSE}
  se[1] ~ dbeta(2, 1)
  sp[1] ~ dbeta(2, 1)
```

To give the model some information that we expect the test characteristics to be closer to 100% than 0%.

. . .

Or we can use stronger priors for one or both tests.


## Priors

A quick way to see the distribution of a prior:

```{r, fig.width=3, fig.height=3}
curve(dbeta(x, 1, 1), from=0, to=1)
qbeta(c(0.025,0.975), shape1=1, shape2=1)
```

---

This was minimally informative, but how does that compare to a weakly informative prior for e.g. sensitivity?

```{r, fig.width=3, fig.height=3}
curve(dbeta(x, 2, 1), from=0, to=1)
qbeta(c(0.025,0.975), shape1=2, shape2=1)
```

---

```{r}
qbeta(c(0.025,0.975), shape1=2, shape2=1)
```

Or more accurately:

```{r}
library("TeachingDemos")
hpd(qbeta, shape1=2, shape2=1)
```

. . .

Credible vs confidence intervals:

  - For MCMC these are usually calculated using highest posterior density (HPD) intervals
  - Therefore there is a difference between:
      - `qbeta(c(0.025,0.975), ...)`
      - `hpd(qbeta, ...)`
  - Technically HPD intervals are credible intervals...
  
---

What about a more informative prior?

```{r, fig.width=3, fig.height=3}
curve(dbeta(x, 20, 2), from=0, to=1)
qbeta(c(0.025,0.975), shape1=20, shape2=2)
hpd(qbeta, shape1=20, shape2=2)
```

## Choosing a prior

What we want is e.g. Beta(20,1)

But typically we have median and 95% confidence intervals from a paper, e.g.:

"The median (95% CI) estimates of the sensitivity and specificity of the shiny new test were 94% (92-96%) and 99% (97-100%) respectively"

. . .

How can we generate a Beta( , ) prior from this?

## The PriorGen package

"The median (95% CI) estimates of the sensitivity and specificity of the shiny new test were 94% (92-96%) and 99% (97-100%)"

```{r}
library("PriorGen")
findbeta(themedian = 0.94, percentile=0.95, percentile.value = 0.92)
hpd(qbeta, shape1=429.95, shape2=27.76)
```
---

```{r}
curve(dbeta(x, shape1=429.95, shape2=27.76))
```

## Initial values

Part of the problem before was also that we were specifying extreme initial values:

```{r}
se <- list(chain1=c(0.01,0.99), chain2=c(0.99,0.01))
sp <- list(chain1=c(0.01,0.99), chain2=c(0.99,0.01))
```

. . .

Let's change these to:

```{r}
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
```


## Analysing simulated data

This is useful to check that we can recover parameter values!

```{r}
# Set a random seed so that the data are reproducible:
set.seed(2021-06-28)

se <- c(0.9, 0.6)
sp <- c(0.95, 0.9)
N <- 1000
prevalence <- 0.5

data <- tibble(Status = rbinom(N, 1, prevalence)) %>%
  mutate(Test1 = rbinom(N, 1, se[1]*Status + (1-sp[1])*(1-Status))) %>%
  mutate(Test2 = rbinom(N, 1, se[2]*Status + (1-sp[2])*(1-Status)))

twoXtwo <- with(data, table(Test1, Test2))
Tally <- as.numeric(twoXtwo)
```

. . .

We know that e.g. the first test has Sensitivity of 90% and Specificity of 95% - so the model *should* be able to tell us that...

# Practical Session 2

## Points to consider {.fragile}

1. What is the typical autocorrelation (and therefore effective sample size) of Hui-Walter models compared to the simpler models we were running earlier?  Is there any practical consequence of this?

1. How does changing the prior distributions for the se and sp of one test affect the inference for the other test parameters?


`r if(params$presentation) {"\\begin{comment}"}`


## Exercise 1 {.fragile}

Simulate some data using the code given above (under "Analysing simulated data"), and run it using the following Hui-Walter model with truncated Beta(1,1) priors for sensitivity and specificity of both tests:

```{r echo=FALSE, comment=''}
hw_definition <- c("model{
  Tally ~ dmulti(prob, N)
  
  # Test1- Test2-
	prob[1] <- (prev * ((1-se[1])*(1-se[2]))) + ((1-prev) * ((sp[1])*(sp[2])))

  # Test1+ Test2-
	prob[2] <- (prev * ((se[1])*(1-se[2]))) + ((1-prev) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob[3] <- (prev * ((1-se[1])*(se[2]))) + ((1-prev) * ((sp[1])*(1-sp[2])))

  # Test1+ Test2+
	prob[4] <- (prev * ((se[1])*(se[2]))) + ((1-prev) * ((1-sp[1])*(1-sp[2])))

  prev ~ dbeta(1, 1)
  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)T(1-sp[2], )
  sp[2] ~ dbeta(1, 1)

  #data# Tally, N
  #monitor# prev, prob, se, sp, deviance
  #inits# prev, se, sp
}
")
cat(hw_definition, sep='', file='hw_truncated.txt')
cleanup <- c(cleanup, 'hw_truncated.txt')
cat(hw_definition)
```

What are the results?

## Solution 1 {.fragile}

```{r}
prev <- list(chain1=0.05, chain2=0.95)
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

results <- run.jags('hw_truncated.txt', n.chains=2)

# Note: this is only commented out to save space in the exercise file!
# plot(results)
# check convergence and effective sample size, and then interpret results:
results
```

Note that the 95% confidence intervals for prev, se and sp are all quite wide, but at least they do contain the simulation values!


## Exercise 2 {.fragile}

- Find beta distribution priors for:

  * Sensitivity = 0.9 (95% CI: 0.85 - 0.95)
  * Specificity = 0.95 (95%CI: 0.92-0.97)

- Look at these distributions using curve and hpd

- Modify your model from exercise 1 using these priors for test 1 (leave the priors for test 2 unchanged)
  - Make sure to name your new model something different, so that you can easily run it using either set of priors for test 1!

- How does this affect the inference for test 2?


## Solution 2 {.fragile}

Parameters for Sensitivity = 0.9 (95% CI: 0.85 - 0.95):


```{r}
PriorGen::findbeta(themean=0.9, percentile = 0.975, percentile.value = 0.85)
hpd(qbeta, shape1=148.43, shape2=16.49)
curve(dbeta(x, 148.43, 16.49), from=0, to=1)
```

Parameters for Specificity = 0.95 (95%CI: 0.92-0.97):

```{r}
PriorGen::findbeta(themean=0.95, percentile = 0.975, percentile.value = 0.92)
hpd(qbeta, shape1=240.03, shape2=12.63)
curve(dbeta(x, 240.03, 12.63), from=0, to=1)
```

Here is the updated model using the new prior values:

```{r echo=FALSE, comment=''}
hw_definition <- c("model{
  Tally ~ dmulti(prob, N)
  
  # Test1- Test2-
	prob[1] <- (prev * ((1-se[1])*(1-se[2]))) + ((1-prev) * ((sp[1])*(sp[2])))

  # Test1+ Test2-
	prob[2] <- (prev * ((se[1])*(1-se[2]))) + ((1-prev) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob[3] <- (prev * ((1-se[1])*(se[2]))) + ((1-prev) * ((sp[1])*(1-sp[2])))

  # Test1+ Test2+
	prob[4] <- (prev * ((se[1])*(se[2]))) + ((1-prev) * ((1-sp[1])*(1-sp[2])))

  prev ~ dbeta(1, 1)
  se[1] ~ dbeta(148.43, 16.49)T(1-sp[1], )
  sp[1] ~ dbeta(240.03, 12.63)
  se[2] ~ dbeta(1, 1)T(1-sp[2], )
  sp[2] ~ dbeta(1, 1)

  #data# Tally, N
  #monitor# prev, prob, se, sp, deviance
  #inits# prev, se, sp
}
")
cat(hw_definition, sep='', file='hw_stronginf.txt')
cleanup <- c(cleanup, 'hw_stronginf.txt')
cat(hw_definition)
```

```{r}
results <- run.jags('hw_stronginf.txt', n.chains=2)

# Note: this is only commented out to save space in the exercise file!
# plot(results)
# check convergence and effective sample size, and then interpret results:
results
```

Note that the 95% confidence intervals are much narrower now, including for test 2!!!


## Exercise 3 {.fragile}

Now adjust the sample size so that you have N=100, re-simulate the data, and re-run the models with both sets of priors.
  - What do you notice about the results compared to N=1000?
  
Also change the prevalence from 50% to 10% or 90%
  - How does this affect your ability to estimate the sensitivity and specificity of test 2 (using strong priors for test 1)?

## Solution 3 {.fragile}

TODO

- Change sample size
  - Wider CI particualrly with weak priors

- Change the simulated prevalence
  - Low prevalence = Sp can be estimated
  - High prevalence = Se can be estimated

## Optional exercise A {.fragile}

Adapt the model so that you can specify the `hyper-priors` of the sensitivity and specificity for test 1 as data

Now pretend that the manufacturer of the test told you that Test 1 actually has these characteristics:

  * Sensitivity = 0.95 (95% CI: 0.92 - 0.98)
  * Specificity = 0.999 (95%CI: 0.99 - 1.00)

Re-estimate the values you would need to use for the priors

Now run your adapted model using these values instead (using the original dataset with N=1000)
  - What effect does the change to Test 1 priors have on the posterior for Test 2?
  - Other than comparing to the simulation parameters (which you would not know in real life!) is there any way that you can tell the priors for test 1 are not realistic?
  
## Optional solution A {.fragile}

TODO

Key points:
  - Test 2 looks worse than it is
  - This is hard to assess in reality other than assessing sensitivty to priors by trial and error


`r if(params$presentation) {"\\end{comment}"}`


## Summary {.fragile}

- Hui-Walter models can do magical things, but:
  - They typically exhibit high autocorrelation
  - They may not converge
  - Need a larger sample for the same effective sample size
  
- More informative priors for one test will
  - Improve identifiability of the model
  - Impact the inference for all other parameters in the model!

```{r include=FALSE}
unlink(cleanup)
```
