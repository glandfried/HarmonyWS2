---
title: Session 1
subtitle: A practical introduction to MCMC
date: "2021-06-28"
author:
  - Matt Denwood
theme: metropolis
aspectratio: 43
colortheme: seahorse
header-includes: 
  - \input{../preamble}
params:
  presentation: TRUE
output:
  beamer_presentation:
      pandoc_args: ["-t", "beamer"]
      slide_level: 2
  html_document: default
---

```{r rendering, eval=FALSE, include=FALSE}
# To render this as PDF (beamer) slides run:
rmarkdown::render('Session_1.Rmd', 'beamer_presentation', params=list(presentation=TRUE))
# And for html:
rmarkdown::render('Session_1.Rmd', 'html_document', params=list(presentation=FALSE))
```

```{r setup, include=FALSE}
library("tidyverse")
set.seed(2021-06-22)

# Reduce the width of R code output for PDF only:
if(params$presentation) options(width=60)
knitr::opts_chunk$set(echo = TRUE)

# Reduce font size of R code output for Beamer:
if(params$presentation){
  knitr::knit_hooks$set(size = function(before, options, envir) {
    if(before){
      knitr::asis_output(paste0("\\", options$size))
    }else{
      knitr::asis_output("\\normalsize")
    }
  })
  knitr::opts_chunk$set(size = "scriptsize")
}

# Collapse successive chunks:
space_collapse <- function(x){ gsub("```\n*```r*\n*", "", x) }
# Reduce space between chunks:
space_reduce <- function(x){ gsub("```\n+```\n", "", x) }
knitr::knit_hooks$set(document = space_collapse)
```

# Course Outline and Practicalities

## Overview

Date/time:

  - 28th June to 1st July 2021
  - 09.00 - 12.30 daily
  - Use the same Zoom link all week!

Teachers:

  - Matt Denwood (University Of Copenhagen)
  - Nils Toft (IQinAbox)
  - SÃ¸ren Saxmose Nielsen (University Of Copenhagen)
  - Maj Beldring Henningsen (University of Copenhagen)


# Motivation

## Diagnostic test evaluation: with gold standard = simple!{.fragile}

`r if(params$presentation) {"\\begin{comment}"}`
```{r}
library("tidyverse")
```
`r if(params$presentation) {"\\end{comment}"}`


```{r}
se <- c(1, 0.6)
sp <- c(1, 0.9)
N <- 1000
prevalence <- 0.25

data <- tibble(Status = rbinom(N, 1, prevalence)) %>%
  mutate(Test_1 = Status) %>%
  mutate(Test_2 = rbinom(N, 1, se[2]*Status + (1-sp[2])*(1-Status)))

(twoXtwo <- with(data, table(Test_1, Test_2)))
```

- The true $Status$ is always the same as the result of $Test_1$
. . .

```{r}
(sensitivity <- twoXtwo[2,2] / sum(twoXtwo[2,1:2]))
(specificity <- twoXtwo[1,1] / sum(twoXtwo[1,1:2]))
```

## Diagnostic test evaluation: no gold standard

- Now we have both values of sensitivity and specificity below 1...

```{r}
se <- c(0.9, 0.6)
sp <- c(0.95, 0.9)
N <- 1000
prevalence <- 0.25

data <- tibble(Status = rbinom(N, 1, prevalence)) %>%
  mutate(Test_1 = rbinom(N, 1, se[1]*Status + (1-sp[1])*(1-Status))) %>%
  mutate(Test_2 = rbinom(N, 1, se[2]*Status + (1-sp[2])*(1-Status)))

data %>% count(Status, Test_1, Test_2)
```


## Diagnostic test evaluation: no gold standard

- In real life we don't know what $Status$ is...

```{r}
twoXtwo <- with(data, table(Test_1, Test_2))

(sensitivity_1 <- twoXtwo[2,2] / sum(twoXtwo[1:2,2]))
(sensitivity_2 <- twoXtwo[2,2] / sum(twoXtwo[2,1:2]))
(specificity_1 <- twoXtwo[1,1] / sum(twoXtwo[1:2,1]))
(specificity_2 <- twoXtwo[1,1] / sum(twoXtwo[1,1:2]))
```
. . .

- So we will *always* under-estimate the Se and Sp of both tests!

## The solution

- We need to assess the sensitivity and specificity of both tests against the true (but unknown) $Status$ of each individual

- This unknown $Status$ is called the `latent class`
  - Therefore we need to run a `latent class model` ...

. . .

- How can we implement a latent class model?
  - Frequentist statistical methods:  possible, but difficult
  - Bayesian statistical methods:  easier and much more commonly done!


## Learning outcomes

By the end of the course you should be able to:

- Understand what a latent class model is, and how they can be used for diagnostic test evaluation

- Run basic latent class models using R and JAGS for real-world problems

- Interpret the results

- Understand the nuances and complexities associated with these types of analysis and the interpretation of the `latent class`

# Revision

## Bayes Rule

Bayes' theorem is at the heart of Bayesian statistics:

$$P(\theta|Y) = \frac{P(\theta)\times P(Y|\theta)}{P(Y)}$$

. . .

Where:  $\theta$ is our parameter value(s);

$Y$ is the data that we have observed;

$P(\theta|Y)$ is the posterior probability of the parameter value(s);

$P(\theta)$ is the prior probability of the parameters;

$P(Y|\theta)$ is the likelihood of the data given the parameters value(s);

$P(Y)$ is the probability of the data, integrated over parameter space.

---

- In practice we usually work with the following:

$$P(\theta|Y) \propto P(\theta)\times P(Y|\theta)$$

. . .

- Our Bayesian posterior is therefore always a combination of the likelihood of the data, and the parameter priors

- But for more complex models the distinction between what is 'data' and 'parameters' can get blurred!

## MCMC

- A way of obtaining a numerical approximation of the posterior

- Highly flexible

- Not inherently Bayesian but most widely used in this context

- Assessing convergence is essential, otherwise we may not be summarising the true posterior

- Our chains are correlated so we need to consider the effective sample size


## Preparation

- *TODO: online poll for how difficult they found the homework!!*

- *TODO: online poll for prior experience with MCMC!!*

- *TODO: online poll for prior experience with LCM!!*

- Any questions so far?  Anything unclear?

- Do we all have R and JAGS installed?

- Can we all access the teaching material from GitHub?

. . .

*Any problems: ask for help during the first practical session!*


# Session 1: A practical introduction to MCMC

## MCMC in Practice

- We can write a Metropolis algorithm but this is complex and inefficient

- There are a number of general purpose langauages that allow us to define the problem and leave the details to the software:

  * WinBUGS/OpenBUGS
    * Bayesian inference Using Gibbs Sampling
  * JAGS
    * Just another Gibbs Sampler
  * Stan
    * Named in honour of Stanislaw Ulam, pioneer of the Monte Carlo method

## JAGS

- JAGS uses the BUGS language

  * This is a declarative (non-procedural) language
  * The order of statements does not matter
  * The compiler converts our model syntax into an MCMC algorithm with appropriately defined likelihood and prior
  * You can only define each variable once!!!

. . .

- Different ways to run JAGS from R:

  - rjags, runjags, R2jags, jagsUI

- See http://runjags.sourceforge.net/quickjags.html
  * This is also in the GitHub folder

---

A simple JAGS model might look like this:

```{r include=FALSE}
model_definition <- "model{
  # Likelihood part:
  Positives ~ dbinom(prevalence, TotalTests)
  
  # Prior part:
  prevalence ~ dbeta(1, 1)
  
  # Hooks for automatic integration with R:
  #data# Positives, TotalTests
  #monitor# prevalence
  #inits# prevalence
}
"
cat(model_definition, file='basicjags.txt')
```

```{r comment='', echo=FALSE}
cat(model_definition, sep='\n')
```

---

There are two model statements:

- The first:
```{r eval=FALSE}
Positives ~ dbinom(prevalence, TotalTests)
```
states that the number of $Positive$ test samples is Binomially distributed with probability parameter $prevalence$ and total trials $TotalTests$

. . .

- The second:
```{r eval=FALSE}
prevalence ~ dbeta(1,1)
```
states that our prior probability distribution for the parameter $prevalence$ is Beta(1,1), which is the same as Uniform(0,1)

. . .

These are very similar to the likelihood and prior functions defined in the preparatory exercise (although this prior is less informative)

---

The other lines in this model:

```{r eval=FALSE}
#data# Positives, TotalTests
#monitor# prevalence
#inits# prevalence
```
are automated hooks that are only used by runjags

. . .

Compared to our Metropolis algorithm, this JAGS model is:

  * Eaiser to write and understand
  * More efficient (lower autocorrelation)
  * Faster to run

---

To run this model, copy/paste the code above into a new text file called "basicjags.txt" in the same folder as your current working directory.  Then run:

```{r}
library('runjags')

# data to be retrieved by runjags
Positives <- 70
TotalTests <- 100

# initial values to be retrieved by runjags:
prevalence <- list(chain1=0.05, chain2=0.95)
```

---

```{r include=FALSE}
runjags.options(silent.jags=TRUE)
```

```{r message=FALSE, warning=FALSE, results='hide'}
results <- run.jags('basicjags.txt', n.chains=2, burnin=5000, sample=10000)
```


First check the plots for convergence:

```{r eval=FALSE, include=TRUE}
plot(results)
```

```{r include=FALSE}
pt <- plot(results)
```

---

Trace plots: the two chains should be stationary:

```{r echo=FALSE}
print(pt[[1]])
```

---

ECDF plots: the two chains should be very close to each other:

```{r echo=FALSE}
print(pt[[2]])
```

---

Histogram of the combined chains should appear smooth:

```{r echo=FALSE}
print(pt[[3]])
```

---

Autocorrelation plot tells you how well behaved the model is:

```{r echo=FALSE}
print(pt[[4]])
```

---

Then check the effective sample size (SSeff) and Gelman-Rubin statistic (psrf):

```{r}
results
```


Reminder:  we want psrf < 1.05 and SSeff > 1000


## Introduction to practical sessions

- Each practical session will consist of:
  1. Some general/philosophical points to consider
  2. One or more practical exercises for everyone to complete
  3. One or more additional (optional) exercise for those that finish the main exercise early
  4. A wrap-up discussion to reinforce the key messages

. . .

- Consideration points are given in the PDF but the exercises (and solutions) are only in the HTML versions

## Introduction to practical sessions

- We have approximately 1 hour per practical session
  - 30-45 minutes for exercises, 15-30 minutes for discussion

. . .

- We will allocate you in pairs / threes to breakout rooms for the exercises (partly randomly, partly based on your institution)
  - If you need help please use the "Ask For Help" feature from your breakout room
  - Otherwise we will drop into the breakout rooms periodically to see how you are getting on!

# Practical Session 1

## Points to consider

1. What are the advantages and disadvantages of Bayesian MCMC relative to more standard frequentist likelihood-based methods?

2. `Identifiability` refers to the ability of a model to extract useful information from a dataset for a particular set of parameters. What 3 things affect whether or not a model/parameter will be identifiable?

. . .

The exercises can be found in Session_1.html!

`r if(params$presentation) {"\\begin{comment}"}`

## Exercise 1 {.fragile}

Follow these steps to run the basic JAGS model given above:

- Create a new text file in RStudio by clicking on `File` then `New File` then `Text file`

- Copy the model definition given above into this text file

- Save the file as `basicjags.txt` in a folder where you can find it again

- Set your R working directory to the same folder, by clicking on `Session` then `Set Working Directory` then `Choose Directory` and choosing the folder location

- Create a new R file in RStudio by clicking on `File` then `New File` then `R script`, and save the file as e.g. `Session 1 exercises.R` in the same folder

- Copy and paste the R code given above into this R script file

- Take your time to play around with this code and make sure that (1) it works, and (2) you know what is going on

- Change the initial values for prevalence to 0.5 in both chains. Does it make a difference to the output?

- Change the number of samples to e.g. 50000 or 100 - what difference does this make?

- Ask for help if you have problems!


### Solution 1 {.fragile}

Your JAGS model (basicjags.txt) should look like this:

```{r comment='', echo=FALSE}
cat(model_definition, sep='\n')
```

And your R code (Session 1 exercises.R) should look like this:

```{r}
library('runjags')

# data to be retrieved by runjags
Positives <- 70
TotalTests <- 100

# initial values to be retrieved by runjags:
prevalence <- list(chain1=0.05, chain2=0.95)

# run the model:
results <- run.jags('basicjags.txt', n.chains=2, burnin=5000, sample=10000)

# check convergence and effective sample size, and then interpret results:
plot(results)
results
```

Convergence is assessed in two ways:  firstly from the trace plots, and secondly from the psrf (Gelman-Rubin statistic).  The effective sample size is SSeff.  Once you are happy that the model has converged and has enough of a sample size, then you can interpret the results (typically median and 95% confidence interval estimates).

We can change the initial values like so:

```{r}
# initial values to be retrieved by runjags:
prevalence <- list(chain1=0.5, chain2=0.5)

# run the model:
results <- run.jags('basicjags.txt', n.chains=2, burnin=5000, sample=10000)

# check convergence and effective sample size, and then interpret results:
plot(results)
results
```

This doesn't make any difference to the inference, because the model converges (and we remove burnin) in either case. The posteriors being independent of the initial values is a key assumption of using MCMC!

Increasing the number of samples to 50000 improves the effective sample size but also takes longer to run.  Reducing the number of samples to 100 results in an effective sample size that is too small and therefore we get unreliable estimates that change every time the model is run!

## Exercise 2 {.fragile}

We can fit the same model using frequentist maximum likelihood methods like this:

```{r}
Positives <- 70
TotalTests <- 100

# We can time how long this takes manually:
system.time({
  # The equivalent model to JAGS:
  freq_model <- glm(cbind(Positives, TotalTests-Positives) ~ 1, family=binomial)
})

# The intercept of the GLM is on the logit scale, so we need to take the inverse logit transform to get the prevalence:
plogis(coef(freq_model))
plogis(confint(freq_model))
```

Compare the results from the JAGS model and the frequentist model:
  - Are there any differences in the inference?
  - Are there any practical differences between JAGS and standard GLM models?
  
Change the priors for the JAGS model from Beta(1,1) to Beta(20,1)
  - How does this affect the inference?


### Solution 2 {.fragile}

*TODO*

Key points:

- JAGS is more flexible but more complex and takes longer to run
- We also have to worry about convergence - this is done for us with GLM
- Bayesian methods can (and must) incorporate priors

## Exercise 3 {.fragile}

Using JAGS allows us to add complexity to basic models, for example including a parameter for true prevalence that is related to observed prevalence by sensitivity and specificity parameters:

```{r include=FALSE}
model_definition <- "model{
  # Likelihood part:
  Positives ~ dbinom(observed_prevalence, TotalTests)
  
  # Intermediate calculations:
  observed_prevalence <- sensitivity * prevalence + (1-specificity) * (1-prevalence)
  
  # Prior part:
  sensitivity ~ dbeta(1, 1)
  specificity ~ dbeta(1, 1)
  prevalence ~ dbeta(1, 1)
  
  # Hooks for automatic integration with R:
  #data# Positives, TotalTests
  #monitor# prevalence, sensitivity, specificity, observed_prevalence
  #inits# prevalence, sensitivity, specificity
}
"
cat(model_definition, file='obsprevmodel.txt')
```

```{r comment='', echo=FALSE}
cat(model_definition, sep='\n')
```

Copy this model to "obsprevmodel.txt" and run it yourself in JAGS.  Here is some additional R code that you will need:

```{r, eval=FALSE}
# initial values to be retrieved by runjags:
prevalence <- list(chain1=0.05, chain2=0.95)
sensitivity <- list(chain1=0.95, chain2=0.05)
specificity <- list(chain1=0.05, chain2=0.95)
```

- Make sure that the chains have converged and that your effective sample size is sufficient

- Look at the posteriors for the 4 parameters of interest.  What do you notice about identifiability?

- Now set the true prevalence parameter to a fixed value of 80% by commenting out the $prevalence ~ dbeta(1, 1)$ line and adding $prevalence <- 0.8$ in the model. What happens to identifiability now?

- Can you do a similar thing by fixing the sensitivity and specificity parameters to values of 0.9 and 0.99?


### Solution 3 {.fragile}

*TODO*

Key points:

- The observed prevalence is identifiable, other parameters are not identifiable unless we fix either true prevalence or Se/Sp (or give them a strong prior)

```{r, eval=FALSE}
library('runjags')

# data to be retrieved by runjags:
Positives <- 70
TotalTests <- 100

# initial values to be retrieved by runjags:
prevalence <- list(chain1=0.05, chain2=0.95)
sensitivity <- list(chain1=0.95, chain2=0.05)
specificity <- list(chain1=0.05, chain2=0.95)

results <- run.jags('obsprevmodel.txt', n.chains=2, burnin=5000, sample=10000)
```




## Optional Exercise A {.fragile}

- Change the number of chains to 1 and 4
  
  * Remember that you will also need to change the initial values
  * What affect does having different numbers of chains have?

- Try using the `run.jags` argument `method='parallel'` - what affect does this have?


### Solution A {.fragile}

The chains argument can be any positive integer, but you need to make sure that the number of initial values provided is consistent.  For example:

```{r}
prevalence <- list(chain1=0.05)
results1 <- run.jags('obsprevmodel.txt', n.chains=1)
results1

prevalence <- list(chain1=0.05, chain2=0.4, chain3=0.6, chain4=0.95)
results4 <- run.jags('obsprevmodel.txt', n.chains=4)
results4
```

There are two differences:  firstly it is not possible to assess the psrf with only 1 chain (and it is harder to assess convergence generally), and secondly the effective sample size is higher with more chains as the samples are pooled (e.g. 10000 samples from 4 chains is 40000 samples).  So more chains is better.

The downside is that more chains take longer to run.  But we can offset this by parallelising:

```{r}
prevalence <- list(chain1=0.05, chain2=0.4, chain3=0.6, chain4=0.95)
results4p <- run.jags('obsprevmodel.txt', n.chains=4, method='parallel')
results4p
```

Each chain is run in parallel, so as long as you have at least as many processors as chains then you will reduce the run time.  [Note: the run time is not much reduced for this example because the model already runs very quickly and there is a small fixed overhead cost with parallelising chains]
  
There are a large number of other options to runjags.  Some highlights:

  - The method can be parallel or background or bgparallel
  - You can use extend.jags to continue running an existing model (e.g. to increase the sample size)
  - You can use coda::as.mcmc.list to extract the underlying MCMC chains
  - Use the summary() method to extract summary statistics
    * See `?summary.runjags` and `?runjagsclass` for more information

## Optional excersie B {.fragile}

There are three other ways of writing this type of model.  We can replace:

```{r, eval=FALSE}
Positives ~ dbinom(observed_prevalence, TotalTests)
```

with:

```{r, eval=FALSE}
for(i in 1:TotalTests){
  Result[i] ~ dbern(observed_prevalence)
}
```

or:

```{r, eval=FALSE}
for(i in 1:TotalTests){
  NegativePositive[i] ~ dcat(observed_probabilities[1:2])
}
observed_probabilities[2] <- observed_prevalence
observed_probabilities[1] <- 1 - observed_prevalence
```

or:

```{r, eval=FALSE}
Tally[1:2] ~ dmulti(observed_probabilities[1:2], TotalTests)

observed_probabilities[2] <- observed_prevalence
observed_probabilities[1] <- 1 - observed_prevalence
```

In each case the data is different:
- $dbern$ needs the response to be a vector of 0 and 1 reflecting test results for individuals, but uses the same probability as dbinom
- $dcat$ needs the response to be a vector of 1 and 2 reflecting the outcome (1=negative, 2=positive) of the test, and uses a pair of probabilities reflecting the probability of being negative and probability of being positive
- $dmulti$ needs the response to be a Tally of the total number of negative tests and positive tests, and uses the same pair of probabilities as $dcat$

Try to implement these models in JAGS, and see what difference it makes to (a) the model inference, and (b) the amount of time that the model takes to run.

What are the advantages and disadvantages of each approach?

### Solution B {.fragile}

*TODO*

Key points:
- dcat and dbern take longer due to the loop
- Having outcomes at individual level means we could incorporate individual-level predictors on prevalence
- dcat and dmulti allow us to extend the number of possible outcomes to more than binary


`r if(params$presentation) {"\\end{comment}"}`

```{r cleanup, include=FALSE}
unlink('basicjags.txt')
unlink('obsprevmodel.txt')
```
