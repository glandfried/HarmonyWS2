---
title: Session 4
subtitle: Multi-test, multi-population models
date: "2021-06-29"
author:
  - Matt Denwood
theme: metropolis
aspectratio: 43
colortheme: seahorse
header-includes: 
  - \input{../rsc/preamble}
params:
  presentation: TRUE
output:
  beamer_presentation:
      pandoc_args: ["-t", "beamer"]
      slide_level: 2
  html_document: default
---

```{r rendering, eval=FALSE, include=FALSE}
# To render this as PDF (beamer) slides run:
rmarkdown::render('Session_3.Rmd', 'beamer_presentation', params=list(presentation=TRUE))
# And for html:
rmarkdown::render('Session_3.Rmd', 'html_document', params=list(presentation=FALSE))
```

```{r setup, include=FALSE}
library("tidyverse")
library("runjags")
set.seed(2021-06-22)

# Reduce the width of R code output for PDF only:
if(params$presentation) options(width=60)
knitr::opts_chunk$set(echo = TRUE)

# Reduce font size of R code output for Beamer:
if(params$presentation){
  knitr::knit_hooks$set(size = function(before, options, envir) {
    if(before){
      knitr::asis_output(paste0("\\", options$size))
    }else{
      knitr::asis_output("\\normalsize")
    }
  })
  knitr::opts_chunk$set(size = "scriptsize")
}

# Collapse successive chunks:
space_collapse <- function(x){ gsub("```\n*```r*\n*", "", x) }
# Reduce space between chunks:
space_reduce <- function(x){ gsub("```\n+```\n", "", x) }
knitr::knit_hooks$set(document = space_collapse)

# To collect temporary filenames:
cleanup <- character(0)
```

## Why stop at two tests?

In *traditional* diagnostic test evaluation, one test is assumed to be a gold standard from which all other tests are evaluated

So it makes no difference if you assess one test at a time or do multiple tests at the same time

Using a latent class model each new test adds new information - so we should analyse all available test results in the same model

## Simulating data

Simulating data using an arbitrary number of independent tests is quite straightforward.

TODO: clean up R code

```{r}
# Parameter values to simulate:
N <- 200
se <- c(0.8, 0.9, 0.95)
sp <- c(0.95, 0.99, 0.95)

Populations <- 2
prevalence <- c(0.25,0.75)
Group <- sample(1:Populations, N, replace=TRUE)
```

---

```{r}
# Ensure replicable data:
set.seed(2020-02-18)

# Simulate the true latent state (which is unobserved in real life):
true <- rbinom(N, 1, prevalence[Group])
# Simulate test results for test 1:
test1 <- rbinom(N, 1, se[1]*true + (1-sp[1])*(1-true))
# Simulate test results for test 2:
test2 <- rbinom(N, 1, se[2]*true + (1-sp[2])*(1-true))
# Simulate test results for test 3:
test3 <- rbinom(N, 1, se[3]*true + (1-sp[3])*(1-true))

simdata <- data.frame(Population=factor(Group), Test1=test1, Test2=test2, Test3=test3)
```


## Model specification

- Like for two tests, except it is now a 2x2x2 table
  
  * We need to take **extreme** care with multinomial tabulation
  
. . .

TODO:  show equations


## Are the tests conditionally independent?

- What do we mean by "conditionally independent?"

  * Independent of each other conditional on the latent state

. . .

- Example:  we have one PCR test for viral DNA and two antibody tests

  * The two antibody tests are more likely to give the same result than the PCR test
  
. . .

- Example:  we have one blood test, one milk test, and one faecal test

  * But the blood and milk test are basically the same test, just on different samples
  * Therefore the blood and milk tests are more likely to give the same result

. . .

- In both situations we have pairwise correlation between some of the tests


## Dealing with correlation

It helps to consider the data simulation as a biological process.  

```{r}
# Parameter values to simulate:
N <- 200
se1 <- 0.8; sp1 <- 0.95
se2 <- 0.9; sp2 <- 0.99
se3 <- 0.95; sp3 <- 0.95

Populations <- 2
prevalence <- c(0.25,0.75)
Group <- rep(1:Populations, each=N)

# Ensure replicable data:
set.seed(2017-11-21)

# The probability of an antibody response given disease:
abse <- 0.8
# The probability of no antibody response given no disease:
absp <- 1 - 0.2
```

---

```{r}
# Simulate the true latent state:
true <- rbinom(N*Populations, 1, prevalence[Group])

# Tests 1 & 2 will be co-dependent on antibody response:
antibody <- rbinom(N*Populations, 1, abse*true + (1-absp)*(1-true))
# Simulate test 1 & 2 results based on this other latent state:
test1 <- rbinom(N*Populations, 1, se1*antibody + (1-sp1)*(1-antibody))
test2 <- rbinom(N*Populations, 1, se2*antibody + (1-sp2)*(1-antibody))

# Simulate test results for the independent test 3:
test3 <- rbinom(N*Populations, 1, se3*true + (1-sp3)*(1-true))

ind3tests <- data.frame(Population=Group, Test1=test1, Test2=test2, Test3=test3)
```

---

```{r}
# The overall sensitivity of the correlated tests is:
abse*se1 + (1-abse)*(1-sp1)
abse*se2 + (1-abse)*(1-sp2)

# The overall specificity of the correlated tests is:
absp*sp1 + (1-absp)*(1-se1)
absp*sp2 + (1-absp)*(1-se2)
```

. . .

We need to think carefully about what we are conditioning on when interpreting sensitivity and specificity!


## Model specification

```{r, eval=FALSE}

	se_prob[1,p] <- prev[p] * ((1-se[1])*(1-se[2])*(1-se[3]) +covse12 +covse13 +covse23)
	sp_prob[1,p] <- (1-prev[p]) * (sp[1]*sp[2]*sp[3] +covsp12 +covsp13 +covsp23)

	se_prob[2,p] <- prev[p] * (se[1]*(1-se[2])*(1-se[3]) -covse12 -covse13 +covse23)
	sp_prob[2,p] <- (1-prev[p]) * ((1-sp[1])*sp[2]*sp[3] -covsp12 -covsp13 +covsp23)

	...
		
	# Covariance in sensitivity between tests 1 and 2:
	covse12 ~ dunif( (se[1]-1)*(1-se[2]) , min(se[1],se[2]) - se[1]*se[2] )
	# Covariance in specificity between tests 1 and 2:
	covsp12 ~ dunif( (sp[1]-1)*(1-sp[2]) , min(sp[1],sp[2]) - sp[1]*sp[2] )

```


## Generating the model

First use template_huiwalter to create a model file:

```{r, results='hide'}
template_huiwalter(ind3tests, 'auto3tihw.bug')
```

Then find the lines for the covariances that we want to activate:

```{r, echo=FALSE, comment=''}
ml <- readLines('auto3tihw.bug')
cat(gsub('\t','',ml[87:92]), sep='\n')
```

---

And edit so it looks like:

```{r, echo=FALSE, comment=''}
ml[87:92] <- c('	# Covariance in sensitivity between Test1 and Test2 tests:', '	covse12 ~ dunif( (se[1]-1)*(1-se[2]) , min(se[1],se[2]) - se[1]*se[2] )  ## if the sensitivity of these tests may be correlated', '	 # covse12 <- 0  ## if the sensitivity of these tests can be assumed to be independent','	# Covariance in specificity between Test1 and Test2 tests:', '	covsp12 ~ dunif( (sp[1]-1)*(1-sp[2]) , min(sp[1],sp[2]) - sp[1]*sp[2] )  ## if the specificity of these tests may be correlated', '	 # covsp12 <- 0  ## if the specificity of these tests can be assumed to be independent')
cat(ml, file='auto3tihw.bug', sep='\n')
ml <- readLines('auto3tihw.bug')
cat(gsub('\t','',ml[87:92]), sep='\n')
```

[i.e. swap the comments around]

---

You will also need to uncomment out the relevant initial values for BOTH chains (on lines 117-122 and 128-133):

```{r, echo=FALSE, comment=''}
ml <- readLines('auto3tihw.bug')
cat(gsub('\t','',ml[128:133]), sep='\n')
```

So that they look like:

```{r, echo=FALSE, comment=''}
ml[c(117,120)] <- c('"covse12" <- 0', '"covsp12" <- 0')
ml[c(128,131)] <- c('"covse12" <- 0', '"covsp12" <- 0')
cat(ml, file='auto3tihw.bug', sep='\n')
ml <- readLines('auto3tihw.bug')
cat(gsub('\t','',ml[128:133]), sep='\n')
ff <- file.copy('auto3tihw.bug', 'auto3tihw2.bug')
```

```{r, results='hide'}
results <- run.jags('auto3tihw.bug')
```




## Practical considerations

- Adds complexity to the model i.e. reduces identifiability

- Probabilities need to be constrained

- We need extreme extreme care to make sure the equations are correct



## Template Hui-Walter

We would usually start with individual-level data in a dataframe:

```{r}
se1 <- 0.9; se2 <- 0.8; sp1 <- 0.95; sp2 <- 0.99
N <- 100
prevalences <- tribble(~Population, ~Prevalence,
                       1, 0.1,
                       2, 0.5,
                       3, 0.9 ) %>%
  mutate(Population = factor(Population, levels=1:3, labels=str_c("Pop_",1:3)))

simdata <- tibble(Population = sample(levels(prevalences$Population), N, replace=TRUE)) %>%
  left_join(prevalences, by="Population") %>%
  mutate(Status = rbinom(N, 1, Prevalence)) %>%
  mutate(Test_1 = rbinom(N, 1, se[1]*Status + (1-sp[1])*(1-Status))) %>%
  mutate(Test_2 = rbinom(N, 1, se[2]*Status + (1-sp[2])*(1-Status)))

```

---

```{r, eval=FALSE}
head(simdata)
```

```{r, echo=FALSE}
head(simdata)
```

[Except that Prevalence and Status would not normally be known!]

---

The model code and data format for an arbitrary number of populations (and tests) can be determined automatically using the template_huiwalter function from the runjas package:

```{r, results='hide'}
template_huiwalter(simdata %>% select(Population, Test_1, Test_2), outfile='autohw.bug')
```

---

This generates self-contained model/data/initial values etc (you can ignore covse and covsp for now):

```{r echo=FALSE, comment=''}
cat(readLines('autohw.bug')[-(1:2)], sep='\n')
```

---

And can be run directly from R:

```{r, results='hide'}
results <- run.jags('autohw.bug')
results
```

```{r echo=FALSE}
res <- summary(results)[,c(1:3,9,11)]
res[] <- round(res, 3)
knitr::kable(res)
```

---

- Modifying priors must still be done directly in the model file

- The model needs to be re-generated if the data changes
  * But remember that your modified priors will be reset

- There must be a single column for the population (as a factor), and all of the other columns (either factor, logical or numeric) are interpreted as being test results

NB:  change priors, turn on/off covariances, add deviance monitor

# Practical session 4

## Points to consider {.fragile}

1. How does including a third test impact the inference for the first two tests?

1. What happens if we include correlation between tests?

1. Can we include correlation if we only have 2 tests?


`r if(params$presentation) {"\\begin{comment}"}`

## Exercise plan TODO

- Give them a dataset with 3 tests where there is correlation between 2 of them
  - Including the 3rd test impacts inference on the other two
  - Including correlation between tests 2 and 3 has an additional impact
  - It is technically possible to fit correlation if just using tests 1 and 2

- Ask them to fit just tests 2 and 3 vs just 1 and 2
  - Prevalence will change
  - Even if using correlation
  - Set up for discussion of latent class tomorrow

Optional:

- Simulate data with 4, 5, 6 etc tests and analyse
  - Observe the non-linear increase in the number of covariance terms
  - What happens if tests 4 and 5 have (near) identical results?

## Exercise {.fragile}

Simulate data with N=1000 and dependence between tests 1 and 2

Then fit a model assuming independence between all tests and compare the results to your simulation parameters

Now turn on covariance between tests 1 and 2 and refit the model.  Are the results more reasonable?


## Solution {.fragile}


```{r}
# Parameter values to simulate:
N <- 1000
se1 <- 0.8
se2 <- 0.9
se3 <- 0.95
sp1 <- 0.95
sp2 <- 0.99
sp3 <- 0.95

# The probability of an antibody response given disease:
abse <- 0.8
# The probability of no antibody response given no disease:
absp <- 1 - 0.2

# Simulate the true latent state:
true <- rbinom(N*Populations, 1, prevalence[Group])

# Tests 1 & 2 will be co-dependent on antibody response:
antibody <- rbinom(N*Populations, 1, abse*true + (1-absp)*(1-true))
# Simulate test 1 & 2 results based on this other latent state:
test1 <- rbinom(N*Populations, 1, se1*antibody + (1-sp1)*(1-antibody))
test2 <- rbinom(N*Populations, 1, se2*antibody + (1-sp2)*(1-antibody))

# Simulate test results for the independent test 3:
test3 <- rbinom(N*Populations, 1, se3*true + (1-sp3)*(1-true))

ind3tests <- data.frame(Population=Group, Test1=test1, Test2=test2, Test3=test3)

# The overall sensitivity of the correlated tests is:
abse*se1 + (1-abse)*(1-sp1)
abse*se2 + (1-abse)*(1-sp2)

# The overall specificity of the correlated tests is:
absp*sp1 + (1-absp)*(1-se1)
absp*sp2 + (1-absp)*(1-se2)

template_huiwalter(ind3tests, outfile='auto3tihw.bug')
```

Then change relevant lines in auto3tihw.bug so that it looks like:

```{r, echo=FALSE, eval=FALSE}
unlink('auto3tihw.bug')
ff <- file.copy('auto3tihw2.bug', 'auto3tihw.bug')
Sys.sleep(1)
```

```{r, echo=FALSE, comment='', eval=FALSE}
cat(readLines('auto3tihw.bug'), sep='\n')
```

Then run the model:

```{r, results='hide', eval=FALSE}
results <- run.jags('auto3tihw.bug')
```

Now check the results:

```{r, eval=FALSE}
results
```

We do a better job of estimating prevalence, and the se/sp for tests 1 and 2 better reflect the overall probability conditional on true status (i.e. corrected for antibody status). But notice that our effective sample size is much smaller than it was! We could run the model for a bit longer:

```{r, results='hide', eval=FALSE}
results <- extend.jags(results)
```

```{r, eval=FALSE}
results
```


## Optional Exercise {.fragile}

Re-fit a model to this data using all three possible covse and covsp parameters

What do you notice about the results?

## Optional Solution {.fragile}

You can either manually change all 3 covse/covsp from before, or regenerate the model using the covariance=TRUE option:

```{r, results='hide'}
template_huiwalter(ind3tests, outfile='auto3tichw.bug', covariance=TRUE)
results <- run.jags('auto3tichw.bug')
```

```{r}
results
```

This chains haven't converged (check out the psrf): the model is (or is close to being) unidentifiable.

`r if(params$presentation) {"\\end{comment}"}`



## Summary {.fragile}

- Including multiple tests is technically easy but philosophically more difficult

- Complexity of adding correlation terms increases with more tests
  - Probably best to stick to correlations with biological justification?
  
- Adding/removing test results may change the posterior for
  - Other test Se / Sp
  - Prevlanece

. . .

Homework:  think about what exactly the latent class is in these situations:

1. An antigen plus antibody test

1. Two antibody tests 

```{r include=FALSE}
unlink(cleanup)
```
