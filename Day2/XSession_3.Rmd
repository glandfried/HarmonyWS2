---
title: Session 2
subtitle: Basic Hui-Walter models
date: "2021-06-28"
author:
  - Matt Denwood
theme: metropolis
aspectratio: 43
colortheme: seahorse
header-includes: 
  - \input{../preamble}
params:
  presentation: TRUE
output:
  beamer_presentation:
      pandoc_args: ["-t", "beamer"]
      slide_level: 2
  html_document: default
---

```{r rendering, eval=FALSE, include=FALSE}
# To render this as PDF (beamer) slides run:
rmarkdown::render('Session_2.Rmd', 'beamer_presentation', params=list(presentation=TRUE))
# And for html:
rmarkdown::render('Session_2.Rmd', 'html_document', params=list(presentation=FALSE))
```

```{r setup, include=FALSE}
library("tidyverse")
set.seed(2021-06-22)

# Reduce the width of R code output for PDF only:
if(params$presentation) options(width=60)
knitr::opts_chunk$set(echo = TRUE)

# Reduce font size of R code output for Beamer:
if(params$presentation){
  knitr::knit_hooks$set(size = function(before, options, envir) {
    if(before){
      knitr::asis_output(paste0("\\", options$size))
    }else{
      knitr::asis_output("\\normalsize")
    }
  })
  knitr::opts_chunk$set(size = "scriptsize")
}

# Collapse successive chunks:
space_collapse <- function(x){ gsub("```\n*```r*\n*", "", x) }
# Reduce space between chunks:
space_reduce <- function(x){ gsub("```\n+```\n", "", x) }
knitr::knit_hooks$set(document = space_collapse)
```

# Session 2:  Basic Hui-Walter models

## Recap

- Fitting models using MCMC is easy with JAGS / runjags

- But we must **never forget** to check convergence and effective sample size!

- More complex models become easy to implement

  * For example imperfect diagnostic tests
  * But remember to be realistic about what is possible with your data

. . .

- So how do we extend these models to multiple diagnostic tests?


## Hui-Walter Model

- A particular model formulation that was originally designed for evaluating diagnostic tests in the absence of a gold standard

- Not necessarily (or originally) Bayesian but often implemented using Bayesian MCMC
  
- But evaluating an imperfect test against another imperfect test is a bit like pulling a rabbit out of a hat
  * If we don't know the true disease status, how can we estimate sensitivity or specificity for either test?


## Model Specification


```{r include=FALSE}
hw_definition <- c("model{
  Tally ~ dmulti(prob, TotalTests)
  
  # Test1- Test2-
	prob[1] <- (prev * ((1-se[1])*(1-se[2]))) + ((1-prev) * ((sp[1])*(sp[2])))

  # Test1+ Test2-
	prob[2] <- (prev * ((se[1])*(1-se[2]))) + ((1-prev) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob[3] <- (prev * ((1-se[1])*(se[2]))) + ((1-prev) * ((sp[1])*(1-sp[2])))
", " 
  # Test1+ Test2+
	prob[4] <- (prev * ((se[1])*(se[2]))) + ((1-prev) * ((1-sp[1])*(1-sp[2])))

  prev ~ dbeta(1, 1)
  se[1] ~ dbeta(1, 1)
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)
  sp[2] ~ dbeta(1, 1)

  #data# Tally, TotalTests
  #monitor# prev, prob, se, sp
  #inits# prev, se, sp
}
")
cat(hw_definition, sep='', file='basic_hw.bug')
```


```{r comment='', echo=FALSE}
cat(hw_definition[1], sep='\n')
```

---

```{r comment='', echo=FALSE}
cat(hw_definition[2], sep='\n')
```

---

```{r}
twoXtwo <- matrix(c(48, 12, 4, 36), ncol=2, nrow=2)
twoXtwo
```


```{r, message=FALSE, warning=FALSE, results='hide'}
library('runjags')

Tally <- as.numeric(twoXtwo)
TotalTests <- sum(Tally)

prev <- list(chain1=0.05, chain2=0.95)
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

results <- run.jags('basic_hw.bug', n.chains=2)
```

[Remember to check convergence and effective sample size!]

---

```{r, eval=FALSE}
results
```

```{r echo=FALSE}
res <- summary(results)[,c(1:3,9,11)]
res[] <- round(res, 3)
knitr::kable(res)
```

- Note the wide confidence intervals!


## Practicalities

- These models need A LOT of data, and/or strong priors for one of the tests

- Convergence is more problematic than usual

- Be **very** vareful with the order of combinations in dmultinom!

- Check your results carefully to ensure they make sense!


## Label Switching

How to interpret a test with Se=0% and Sp=0%?

. . .

  * The test is perfect - we are just holding it upside down...

. . .

We can force se+sp >= 1:

```{r eval=FALSE}
  se[1] ~ dbeta(1, 1)
  sp[1] ~ dbeta(1, 1)T(1-se[1], )
```

Or:

```{r eval=FALSE}
  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
```

But not both!

This allows the test to be useless, but not worse than useless.

Alternatively we can have the weakly informative priors:

```{r eval=FALSE}
  se[1] ~ dbeta(2, 1)
  sp[1] ~ dbeta(2, 1)
```

To give the model some information that we expect the test characteristics to be closer to 100% than 0%.

## Simulating data

Analysing simulated data is useful to check that we can recover parameter values.

```{r}
se1 <- 0.9; sp1 <- 0.95;
se2 <- 0.8; sp2 <- 0.99
prevalence <- 0.5; N <- 100

truestatus <- rbinom(N, 1, prevalence)
Test1 <- rbinom(N, 1, (truestatus * se1) + ((1-truestatus) * (1-sp1)))
Test2 <- rbinom(N, 1, (truestatus * se2) + ((1-truestatus) * (1-sp2)))

twoXtwo <- table(Test1, Test2)
Tally <- as.numeric(twoXtwo)
```

Can we recover these parameter values?

## Exercise {.fragile}

Modify the code in the Hui Walter model to force tests to be no worse than useless

Simulate data and recover parameters for:

  * N=10
  * N=100
  * N=1000

`r if(params$presentation) {"\\begin{comment}"}`

## Solution {.fragile}

Model definition:

```{r include=FALSE}
hw_definition <- "model{
  Tally ~ dmulti(prob, TotalTests)
  
  # Test1- Test2-
	prob[1] <- (prev * ((1-se[1])*(1-se[2]))) + ((1-prev) * ((sp[1])*(sp[2])))

  # Test1+ Test2-
	prob[2] <- (prev * ((se[1])*(1-se[2]))) + ((1-prev) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob[3] <- (prev * ((1-se[1])*(se[2]))) + ((1-prev) * ((sp[1])*(1-sp[2])))

  # Test1+ Test2+
	prob[4] <- (prev * ((se[1])*(se[2]))) + ((1-prev) * ((1-sp[1])*(1-sp[2])))
 
  prev ~ dbeta(1, 1)
  se[1] ~ dbeta(HPSe[1,1], HPSe[1,2])T(1-sp[1], )
  sp[1] ~ dbeta(HPSp[1,1], HPSp[1,2])
  se[2] ~ dbeta(HPSe[2,1], HPSe[2,2])T(1-sp[2], )
  sp[2] ~ dbeta(HPSp[2,1], HPSp[2,2])

  #data# Tally, TotalTests, HPSe, HPSp
  #monitor# prev, prob, se, sp
  #inits# prev, se, sp
}
"
cat(hw_definition, file='basic_hw.bug')
```

```{r comment='', echo=FALSE}
cat(hw_definition, sep='\n')
```

Note that we specify the prior hyperparameters as data so we can change these from R without havÃ­ng to edit the model file (this is optional!)

```{r}
se1 <- 0.9
sp1 <- 0.95
se2 <- 0.8
sp2 <- 0.99
prevalence <- 0.5

# Change N to be 10, 100 or 1000:
N <- 100

truestatus <- rbinom(N, 1, prevalence)
Test1 <- rbinom(N, 1, (truestatus * se1) + ((1-truestatus) * (1-sp1)))
Test2 <- rbinom(N, 1, (truestatus * se2) + ((1-truestatus) * (1-sp2)))

twoXtwo <- table(Test1, Test2)
twoXtwo

library('runjags')

Tally <- as.numeric(twoXtwo)
TotalTests <- sum(Tally)
HPSe <- matrix(c(1,1,1,1), nrow=2, ncol=2)
HPSp <- matrix(c(1,1,1,1), nrow=2, ncol=2)

prev <- list(chain1=0.05, chain2=0.95)
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

results <- run.jags('basic_hw.bug', n.chains=2)
results
```

How well do we recover our parameters?

```{r}
se1
se2
sp1
sp2
```

Not that well!

`r if(params$presentation) {"\\end{comment}"}`


## Optional Exercise {.fragile}

Compare results with the following priors for test 1:

  * Sensitivity = 0.9 (95% CI: 0.85 - 0.95)
  * Specificity = 0.95 (95%CI: 0.92-0.97)

[These are the same as in session 1]

`r if(params$presentation) {"\\begin{comment}"}`

## Optional Solution {.fragile}

```{r}
HPSe[1,] <- c(148.43, 16.49)
HPSp[1,] <- c(240.03, 12.63)

HPSe
HPSp

results <- run.jags('basic_hw.bug', n.chains=2)
results
```

How well do we recover our parameters for test 2?

```{r}
se1
se2
sp1
sp2
```

A bit better!  But note that the confidence interval for test 1 is not much narrower than that of the prior:

```{r}
# Sensitivity:
qbeta(c(0.025, 0.5, 0.975), 148.43, 16.49)
# Specificity:
qbeta(c(0.025, 0.5, 0.975), 240.03, 12.63)
```

So we have not gained any additional information about test 1.

`r if(params$presentation) {"\\end{comment}"}`

# Session 2b:  Hui-Walter models for 2 tests and N populations

## Hui-Walter models with multiple populations

- Basically an extension of the single-population model

- Works best with multiple populations each with differing prevalences
  * Including an unexposed population works well
  * BUT be wary of assumptions regarding constant sensitivity/specificity across populations with very different types of infections


## Independent intercepts for populations

```{r eval=FALSE}
model{
  for(p in 1:Populations){
    Tally[1:4, p] ~ dmulti(prob[1:4, p], TotalTests[p])
    # Test1- Test2- Pop1
	  prob[1, p] <- (prev[p] * ((1-se[1])*(1-se[2]))) + ((1-prev[p]) * ((sp[1])*(sp[2])))
    ## etc ##
    prev[p] ~ dbeta(1, 1)
  }

  se[1] ~ dbeta(HPSe[1,1], HPSe[1,2])T(1-sp[1], )
  sp[1] ~ dbeta(HPSp[1,1], HPSp[1,2])
  se[2] ~ dbeta(HPSe[2,1], HPSe[2,2])T(1-sp[2], )
  sp[2] ~ dbeta(HPSp[2,1], HPSp[2,2])

  #data# Tally, TotalTests, Populations, HPSe, HPSp
  #monitor# prev, prob, se, sp
  #inits# prev, se, sp
}
```


## Auto Hui-Walter

We would usually start with individual-level data in a dataframe:

```{r}
se1 <- 0.9; se2 <- 0.8; sp1 <- 0.95; sp2 <- 0.99
prevalences <- c(0.1, 0.5, 0.9)
N <- 100

simdata <- data.frame(Population = sample(seq_along(prevalences), N, replace=TRUE))
simdata$probability <- prevalences[simdata$Population]
simdata$truestatus <- rbinom(N, 1, simdata$probability)
simdata$Test1 <- rbinom(N, 1, (simdata$truestatus * se1) + ((1-simdata$truestatus) * (1-sp1)))
simdata$Test2 <- rbinom(N, 1, (simdata$truestatus * se2) + ((1-simdata$truestatus) * (1-sp2)))
```

---

```{r, eval=FALSE}
head(simdata)
```

```{r, echo=FALSE}
head(simdata)
```

[Except that probability and truestatus would not normally be known!]

---

The model code and data format for an arbitrary number of populations (and tests) can be determined automatically

There is a function (provided in the GitHub repo) that can do this for us:

```{r, results='hide'}
simdata$Population <- factor(simdata$Population, levels=seq_along(prevalences), labels=paste0('Pop_', seq_along(prevalences)))

source("autohuiwalter.R")
auto_huiwalter(simdata[,c('Population','Test1','Test2')], outfile='autohw.bug')
```

---

This generates self-contained model/data/initial values etc (ignore covse and covsp for now):

```{r echo=FALSE, comment=''}
cat(readLines('autohw.bug')[-(1:2)], sep='\n')
```

---

And can be run directly from R:

```{r, results='hide'}
results <- run.jags('autohw.bug')
results
```

```{r echo=FALSE}
res <- summary(results)[,c(1:3,9,11)]
res[] <- round(res, 3)
knitr::kable(res)
```

---

- Modifying priors must still be done directly in the model file

- The model needs to be re-generated if the data changes
  * But remember that your modified priors will be reset

- There must be a single column for the population (as a factor), and all of the other columns (either factor, logical or numeric) are interpreted as being test results

- The function will soon be included in the runjags package
  * Feedback welcome!


## Observation-level model specification

```{r include=FALSE}
glmhw_definition <- c("model{

  for(i in 1:N){
    Status[i] ~ dcat(prob[i, ])
  
	  prob[i,1] <- (prev[i] * ((1-se[1])*(1-se[2]))) + 
	              ((1-prev[i]) * ((sp[1])*(sp[2])))
	  prob[i,2] <- (prev[i] * ((se[1])*(1-se[2]))) + 
	              ((1-prev[i]) * ((1-sp[1])*(sp[2])))
	  prob[i,3] <- (prev[i] * ((1-se[1])*(se[2]))) + 
	              ((1-prev[i]) * ((sp[1])*(1-sp[2])))
	  prob[i,4] <- (prev[i] * ((se[1])*(se[2]))) + 
	              ((1-prev[i]) * ((1-sp[1])*(1-sp[2])))
	  
	  logit(prev[i]) <- intercept + population_effect[Population[i]]
  }
", "
  intercept ~ dnorm(0, 0.33)
  population_effect[1] <- 0
  for(p in 2:Pops){
    population_effect[p] ~ dnorm(0, 0.1)
  }
  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)T(1-sp[2], )
  sp[2] ~ dbeta(1, 1)

  #data# Status, N, Population, Pops
  #monitor# intercept, population_effect, se, sp
  #inits# intercept, population_effect, se, sp
}
")
cat(glmhw_definition, sep='', file='glm_hw.bug')
```


```{r comment='', echo=FALSE}
cat(glmhw_definition[1], sep='\n')
```

---

```{r comment='', echo=FALSE}
cat(glmhw_definition[2], sep='\n')
```

---

- The main difference is the prior for prevalence in each population

- We also need to give initial values for `intercept` and `population_effect` rather than `prev`, and tell `run.jags` the data frame from which to extract the data (except `N` and `Pops`):

```{r, results='hide'}
intercept <- list(chain1=-1, chain2=1)
population_effect <- list(chain1=c(NA, 1, -1), chain2=c(NA, -1, 1))
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

simdata$Status <- with(simdata, factor(interaction(Test1, Test2), levels=c('0.0','1.0','0.1','1.1')))
N <- nrow(simdata)
Pops <- length(levels(simdata$Population))
glm_results <- run.jags('glm_hw.bug', n.chains=2, data=simdata)
```

---

Also like in session 1, the estimates for se/sp should be similar, although this model runs more slowly.

```{r echo=FALSE}
if(!params$presentation){
  cat('Results from the HW model:\n\n\n')
  results
}
```

```{r echo=FALSE}
if(!params$presentation){
  cat('Results from the GLM model:\n\n\n')
  glm_results
}
```

Note:  this model could be used as the basis for adding covariates

For a handy way to generate a GLM model see runjags::template.jags

  * Look out for integration with autohuiwalter in the near (ish) future...


## Exercise

Play around with the autohuiwalter function

Notice the model and data and initial values are in a self contained file

Ignore the covse and covsp for now

[There is no particular solution to this exercise!]


## Summary

- Estimating sensitivity and specificity is like pulling a rabbit out of a hat

- Multiple populations helps **a lot**

- Strong priors for one of the tests helps even more!

- Make sure you tabulate the data correctly ... or use the automated model generator!


```{r cleanup, include=FALSE}
unlink('autohw.bug')
unlink('basic_hw.bug')
unlink('glm_hw.bug')
```


# Practical Session 2

# Session 1b:  Working with basic models (apparent prevalence)

## Other runjags options

There are a large number of other options to runjags.  Some highlights:

  - The method can be parallel or background or bgparallel
  - You can use extend.jags to continue running an existing model (e.g. to increase the sample size)
  - You can use coda::as.mcmc.list to extract the underlying MCMC chains
  - Use the summary() method to extract summary statistics
    * See `?summary.runjags` and `?runjagsclass` for more information

## Using embedded character strings

- For simple models we might not want to bother with an external text file.  Then we can do:

```{r results='hide'}
mt <- "
model{
  Positives ~ dbinom(prevalence, TotalTests)
  prevalence ~ dbeta(2, 2)
  
  #data# Positives, TotalTests
  #monitor# prevalence
  #inits# prevalence
}
"

results <- run.jags(mt, n.chains=2)
```

- But I would advise that you stick to using a separate text file!

## Setting the RNG seed

- If we want to get numerically replicable results we need to add `.RNG.name` and `.RNG.seed` to the initial values, and an additional `#modules#` lecuyer hook to our basicjags.bug file:

```{r, eval=FALSE}
model{
  Positives ~ dbinom(prevalence, TotalTests)
  prevalence ~ dbeta(2, 2)
  
  #data# Positives, TotalTests
  #monitor# prevalence
  #inits# prevalence, .RNG.name, .RNG.seed
  #modules# lecuyer
}
```


```{r, eval=FALSE}
.RNG.name <- "lecuyer::RngStream"
.RNG.seed <- list(chain1=1, chain2=2)
results <- run.jags('basicjags.bug', n.chains=2)
```

- Every time this model is run the results will now be identical

## A different prior

- A quick way to see the distribution of a prior:

```{r, fig.width=3, fig.height=3}
curve(dbeta(x, 2, 2), from=0, to=1)
```

---

- A minimally informative prior might be:

```{r, fig.width=3, fig.height=3}
curve(dbeta(x, 1, 1), from=0, to=1)
```

---

- Let's change the prior we are using to `dbeta(1,1)`:

```{r include=FALSE}
mininf_definition <- "model{
  Positives ~ dbinom(prevalence, TotalTests)
  prevalence ~ dbeta(1, 1)
  
  # Hooks for automatic integration with R:
  #data# Positives, TotalTests
  #monitor# prevalence
  #inits# prevalence
}
"
cat(mininf_definition, file='basicjags.bug')
```

```{r comment='', echo=FALSE}
cat(mininf_definition, sep='\n')
```


## An Equivalent Model

- We could equivalently specify an observation-level model:

```{r include=FALSE}
loop_definition <- "model{
  # Likelihood part:
  for(i in 1:TotalTests){
    Status[i] ~ dbern(prevalence)
  }

  # Prior part:
  prevalence ~ dbeta(1, 1)
  
  # Hooks for automatic integration with R:
  #data# Status, TotalTests
  #monitor# prevalence
  #inits# prevalence
}
"
cat(loop_definition, file='basicloop.bug')
```

```{r comment='', echo=FALSE}
cat(loop_definition, sep='\n')
```

- But we need the data in a different format:  a vector of 0/1 rather than total positives!

```{r}
Positives <- 70
TotalTests <- 100
Status <- c(rep(0, TotalTests-Positives), rep(1, Positives))
```


## A GLM Model

```{r include=FALSE}
glm_definition <- "model{
  # Likelihood part:
  for(i in 1:TotalTests){
    Status[i] ~ dbern(predicted[i])
    logit(predicted[i]) <- intercept
  }

  # Prior part:
  intercept ~ dnorm(0, 10^-6)
  
  # Derived parameter:
  prevalence <- ilogit(intercept)
  
  # Hooks for automatic integration with R:
  #data# Status, TotalTests
  #monitor# intercept, prevalence
  #inits# intercept
}
"
cat(glm_definition, file='basicglm.bug')
```

```{r comment='', echo=FALSE}
cat(glm_definition, sep='\n')
```

---

- This is the start of a generalised linear model, where we could add covariates at individual animal level.

- We introduce a new distribution `dnorm()` - notice this is mean and precision, not mean and sd!

- For a complete list of the distributions available see:
  * https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/
  * This document is also provided on the GitHub repository

- However, notice that the prior is specified differently...


## Exercise {.fragile}

- Run the original version, the observation-level version, and the GLM version of the model and compare results with the same data

- Now try a larger sample size:  e.g. 70 positives out of 100 tests - are the posteriors from the two models more or less similar than before?

- Now try running the GLM model with a prior of `dnorm(0, 0.33)` (and the original data) - does this make a difference?


`r if(params$presentation) {"\\begin{comment}"}`


## Solution {.fragile}

In basicjags.bug:

```{r comment='', echo=FALSE}
cat(mininf_definition, sep='\n')
```

In basicloop.bug:

```{r comment='', echo=FALSE}
cat(loop_definition, sep='\n')
```

In basicglm.bug:

```{r comment='', echo=FALSE}
cat(glm_definition, sep='\n')
```

Comparison:

```{r}
# Data:
Positives <- 7
TotalTests <- 10

# initial values for the basic and loop models:
prevalence <- list(chain1=0.05, chain2=0.95)

# initial values for the glm model:
intercept <- list(chain1=-2, chain2=2)

basicjags <- run.jags('basicjags.bug', n.chains=2)

# Data for the loop and glm models:
Status <- c(rep(0, TotalTests-Positives), rep(1, Positives))

basicloop <- run.jags('basicloop.bug', n.chains=2)
basicglm <- run.jags('basicglm.bug', n.chains=2)
```

Ensure convergence and sample size, then compare:

```{r}
basicjags
basicloop
basicglm
```

The GLM model has slightly different results, due to the prior.

What about a larger dataset:


```{r}
# Data:
Positives <- 70
TotalTests <- 100
Status <- c(rep(0, TotalTests-Positives), rep(1, Positives))

basicjags <- run.jags('basicjags.bug', n.chains=2)
basicloop <- run.jags('basicloop.bug', n.chains=2)
basicglm <- run.jags('basicglm.bug', n.chains=2)
```

Ensure convergence and sample size, then compare:

```{r}
basicjags
basicloop
basicglm
```

The results are more similar, as the posterior is more dominated by the data.

What about a different prior for the GLM model?  In basicglm2.bug:

```{r include=FALSE}
glm_definition <- "model{
  # Likelihood part:
  for(i in 1:TotalTests){
    Status[i] ~ dbern(predicted[i])
    logit(predicted[i]) <- intercept
  }

  # Prior part:
  intercept ~ dnorm(0, 0.33)
  
  # Derived parameter:
  prevalence <- ilogit(intercept)
  
  # Hooks for automatic integration with R:
  #data# Status, TotalTests
  #monitor# intercept, prevalence
  #inits# intercept
}
"
cat(glm_definition, file='basicglm2.bug')
```

```{r comment='', echo=FALSE}
cat(glm_definition, sep='\n')
```

```{r}
# Data:
Positives <- 7
TotalTests <- 10
Status <- c(rep(0, Positives), rep(1, TotalTests-Positives))

basicjags <- run.jags('basicjags.bug', n.chains=2)
basicloop <- run.jags('basicloop.bug', n.chains=2)
basicglm2 <- run.jags('basicglm2.bug', n.chains=2)
```

Ensure convergence and sample size, then compare:

```{r}
basicjags
basicloop
basicglm2
```

The GLM model is now more similar to the others, because the prior for prevalence is more similar.

`r if(params$presentation) {"\\end{comment}"}`


## Optional Exercise {.fragile}

Another way of comparing different priors is to run different models with no data - as there is no influence of a likelihood, the posterior will then be identical to the priors (and the model will run faster).

One way to do this is to make all of the response data (i.e. either Positives or Status) missing.  Try doing this for the following three models, and compare the priors for prevalence:

  - The original model with prior `prevalence ~ dbeta(1,1)`
  - The GLM model with prior `intercept ~ dnorm(0, 10^-6)`
  - The GLM model with prior `intercept ~ dnorm(0, 0.33)`


`r if(params$presentation) {"\\begin{comment}"}`

## Optional Solution {.fragile}

This is the easiest way to remove data from the model without adjusting the model code itself:

```{r}
Positives <- NA
Positives
Status[] <- NA
Status
```

Then we can run the models:

```{r}
basicjags <- run.jags('basicjags.bug', n.chains=2)
basicloop <- run.jags('basicloop.bug', n.chains=2)
basicglm <- run.jags('basicglm.bug', n.chains=2)
basicglm2 <- run.jags('basicglm2.bug', n.chains=2)
```

No need to ensure convergence and sample size as we have no data!  Just compare the priors directly:

```{r}
basicjags
basicloop
basicglm
basicglm2
```

You could also look at plots (particularly the histogram plot) if you wanted to.

`r if(params$presentation) {"\\end{comment}"}`


# Session 1c:  Basics of latent-class models (imperfect test)

## Imperfect tests

- Up to now we have ignored issues of diagnostic test sensitivity and specificity

- Usually, however, we do not have a perfect test, so we do not know how many are truly positive or truly negative, rather than just testing positive or negative.

- But we know that:
$$Prev_{obs} = (Prev_{true}\times Se) + ((1-Prev_{true})\times (1-Sp))$$
$$\implies Prev_{true} = \frac{Prev_{obs}-(1-Sp)}{Se-(1-Sp)}$$


## Model Specification

- We can incorporate the imperfect sensitivity and specicifity into our model:

```{r include=FALSE}
imperfect_definition <- "model{
  Positives ~ dbinom(obsprev, TotalTests)
  obsprev <- (prevalence * se) + ((1-prevalence) * (1-sp))
  
  prevalence ~ dbeta(1, 1)
  se ~ dbeta(1, 1)
  sp ~ dbeta(1, 1)
  
  #data# Positives, TotalTests
  #monitor# prevalence, obsprev, se, sp
  #inits# prevalence, se, sp
}
"
cat(imperfect_definition, file='basicimperfect.bug')
```

```{r comment='', echo=FALSE}
cat(imperfect_definition, sep='\n')
```

---

- And run it:

```{r}
prevalence <- list(chain1=0.05, chain2=0.95)
se <- list(chain1=0.5, chain2=0.99)
sp <- list(chain1=0.5, chain2=0.99)
Positives <- 70
TotalTests <- 100
results <- run.jags('basicimperfect.bug', n.chains=2)
```

[Remember to check convergence and effective sample size!]

---

What do these results tell us?

```{r echo=FALSE}
res <- summary(results)[,c(1:3,9,11)]
res[] <- round(res, 3)
knitr::kable(res)
```


. . .

  * We can estimate the observed prevalence quite well
  * But not the prevalence, se or sp!
    * The model is unidentifiable.

## Priors

- We cannot estimate `se`, `sp` and `prevalence` simultaneously
  
  * We need strong priors for se and sp

- We can use the PriorGen package to generate Beta priors based on published results, for example:

```{r}
PriorGen::findbeta(themean=0.9, percentile = 0.975, percentile.value = 0.8)
```

---

```{r}
qbeta(c(0.025, 0.5, 0.975), 41.82, 4.65)
curve(dbeta(x, 41.82, 4.65), from=0, to=1)
```


## Exercise {.fragile}

- Find beta distribution priors for:

  * Sensitivity = 0.9 (95% CI: 0.85 - 0.95)
  * Specificity = 0.95 (95%CI: 0.92-0.97)

- Look at these distributions using curve and qbeta

- Modify the imperfect test model using these priors and re-estimate prevalence


`r if(params$presentation) {"\\begin{comment}"}`

## Solution {.fragile}

Parameters for Sensitivity = 0.9 (95% CI: 0.85 - 0.95):


```{r}
PriorGen::findbeta(themean=0.9, percentile = 0.975, percentile.value = 0.85)
qbeta(c(0.025, 0.5, 0.975), 148.43, 16.49)
curve(dbeta(x, 148.43, 16.49), from=0, to=1)
```

Parameters for Specificity = 0.95 (95%CI: 0.92-0.97):

```{r}
PriorGen::findbeta(themean=0.95, percentile = 0.975, percentile.value = 0.92)
qbeta(c(0.025, 0.5, 0.975), 240.03, 12.63)
curve(dbeta(x, 240.03, 12.63), from=0, to=1)
```

Updated model:


```{r include=FALSE}
imperfect_definition <- "model{
  Positives ~ dbinom(obsprev, TotalTests)
  obsprev <- (prevalence * se) + ((1-prevalence) * (1-sp))
  
  prevalence ~ dbeta(1, 1)
  se ~ dbeta(148.43, 16.49)
  sp ~ dbeta(240.03, 12.63)
  
  #data# Positives, TotalTests
  #monitor# prevalence, obsprev, se, sp
  #inits# prevalence, se, sp
}
"
cat(imperfect_definition, file='basicimperfect.bug')
```

```{r comment='', echo=FALSE}
cat(imperfect_definition, sep='\n')
```

Results:

```{r}
prevalence <- list(chain1=0.05, chain2=0.95)
se <- list(chain1=0.5, chain2=0.99)
sp <- list(chain1=0.5, chain2=0.99)
Positives <- 70
TotalTests <- 100
results <- run.jags('basicimperfect.bug', n.chains=2, burnin=0, sample=10000)
results
```

This time we get sensible estimates for `prevalence`.

`r if(params$presentation) {"\\end{comment}"}`

## Optional Exercise {.fragile}

- Run the same model with se and sp fixed to the mean estimate

  * How does this affect CI for prevalence?

- Run the same model with se and sp fixed to 1

  * How does this affect estimates and CI for prevalence?


`r if(params$presentation) {"\\begin{comment}"}`

## Optional Solution {.fragile}

Same model with se and sp fixed to the mean estimates:

```{r include=FALSE}
imperfect_definition <- "model{
  Positives ~ dbinom(obsprev, TotalTests)
  obsprev <- (prevalence * se) + ((1-prevalence) * (1-sp))
  
  prevalence ~ dbeta(1, 1)
  se <- 0.9
  # se ~ dbeta(148.43, 16.49)
  sp <- 0.95
  # sp ~ dbeta(240.03, 12.63)
  
  #data# Positives, TotalTests
  #monitor# prevalence, obsprev, se, sp
  #inits# prevalence
}
"
cat(imperfect_definition, file='basicimperfect.bug')
```

```{r comment='', echo=FALSE}
cat(imperfect_definition, sep='\n')
```

Note that we have to remove se and sp from the initial values.  Results:

```{r}
results <- run.jags('basicimperfect.bug', n.chains=2, burnin=0, sample=10000)
results
```

Estimates for `prevalence` are a little bit more precise.

Same model with se and sp fixed to one:

```{r include=FALSE}
imperfect_definition <- "model{
  Positives ~ dbinom(obsprev, TotalTests)
  obsprev <- (prevalence * se) + ((1-prevalence) * (1-sp))
  
  prevalence ~ dbeta(1, 1)
  se <- 1
  # se ~ dbeta(148.43, 16.49)
  sp <- 1
  # sp ~ dbeta(240.03, 12.63)
  
  #data# Positives, TotalTests
  #monitor# prevalence, obsprev, se, sp
  #inits# prevalence
}
"
cat(imperfect_definition, file='basicimperfect.bug')
```

```{r comment='', echo=FALSE}
cat(imperfect_definition, sep='\n')
```

Results:

```{r}
results <- run.jags('basicimperfect.bug', n.chains=2, burnin=0, sample=10000)
results
```

Biased estimates for `prevalence`!

`r if(params$presentation) {"\\end{comment}"}`


## Summary

- Using JAGS / runjags allows us to work with MCMC more easily, safely and efficiently than writing our own sampling algorithms

- But we must *never forget* to check convergence and effective sample size!

- More complex models become easy to implement

  * For example imperfect diagnostic tests

- But just because a model can be defined does not mean that it will be useful for our data

  * We need to be realistic about the information available in the data, what parameters are feasible to estimate, and where we will need to use strong priors
