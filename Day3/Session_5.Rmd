---
title: Session 5
subtitle: How to interpret the latent class
date: "2021-06-30"
author:
  - Matt Denwood
theme: metropolis
aspectratio: 43
colortheme: seahorse
header-includes: 
  - \input{../rsc/preamble}
params:
  presentation: TRUE
output:
  beamer_presentation:
      pandoc_args: ["-t", "beamer"]
      slide_level: 2
  html_document: default
---

```{r rendering, eval=FALSE, include=FALSE}
# To render this as PDF (beamer) slides run:
rmarkdown::render('Session_5.Rmd', 'beamer_presentation', params=list(presentation=TRUE))
# And for html:
rmarkdown::render('Session_5.Rmd', 'html_document', params=list(presentation=FALSE))
```

```{r setup, include=FALSE}
library("tidyverse")
library("runjags")
set.seed(2021-06-22)

# Reduce the width of R code output for PDF only:
if(params$presentation) options(width=60)
knitr::opts_chunk$set(echo = TRUE)

# Reduce font size of R code output for Beamer:
if(params$presentation){
  knitr::knit_hooks$set(size = function(before, options, envir) {
    if(before){
      knitr::asis_output(paste0("\\", options$size))
    }else{
      knitr::asis_output("\\normalsize")
    }
  })
  knitr::opts_chunk$set(size = "scriptsize")
}

# Collapse successive chunks:
space_collapse <- function(x){ gsub("```\n*```r*\n*", "", x) }
# Reduce space between chunks:
space_reduce <- function(x){ gsub("```\n+```\n", "", x) }
knitr::knit_hooks$set(document = space_collapse)
```

# Session 5:  How to interpret the latent class

## Recap

- Adding more populations and more tests to a Hui-Walter model is technically easy
  - Particualrly if using template_huiwalter
  
- Verifying that the assumptions you are making are correct is harder
  - The sensitivity and specificity must be consistent across populations
  - Pairwise correlation between tests should be accounted for (with >2 tests)


## What exactly is our latent class?

Homework (reminder):  think about what exactly the latent class is in these situations:

1. An antigen plus antibody test

. . .

  * The latent status is probably close to the true disease status
  
. . .

1. Two antibody tests 

. . .

  * The latent status is actually 'producing antibodies' not 'diseased'
  
. . .

- What do we mean by "conditionally independent" (revisited) ?

  * Independent of each other conditional on the latent state
  * But the latent state is NOT always *disease*

. . .

- i.e. we're pulling **something** out of a hat, and deciding to call it a rabbit


## A hierarchy of latent states

![DAG with 3 tests and 2 intermediate states](../rsc/dag3test.pdf)

## Branching of processes leading to test results

- Sometimes we have multiple tests that are detecting a similar thing
  
  - For example:  two antibody tests and one antigen test
  - The antibody tests will be correlated
  
. . .

- Or even three antibody tests where two are primed to detect the same thing, and one has a different target!
  
  - In this case all three tests are correlated, but two are more strongly correlated

## Data simulation


## Optional Exercise Code {.fragile}

```{r, eval=FALSE}
# Probability of antibody response conditional on disease status (really bad to illustrate the point):
se_antibody <- 0.5
sp_antibody <- 0.75
N <- 100
# Otherwise the parameters are as before

# True latent infection status as before:
true <- rbinom(N, 1, prevalence[Group])

# Latent class of antibody response conditional on the true status:
antibody <- rbinom(N, 1, se_antibody*true + (1-sp_antibody)*(1-true))

# Simulate test results for test 1 conditional on antibody status:
test1 <- rbinom(N, 1, se1*antibody + (1-sp1)*(1-antibody))
# etc

# Note that the overall sensitivity and specificity of the tests needs to be corrected for the antibody positive step:
overall_se1 <- se_antibody*se1 + (1-se_antibody)*(1-sp1)
overall_sp1 <- sp_antibody*sp1 + (1-sp_antibody)*(1-se1)
# etc
```


## Optional Solution {.fragile}


```{r, results='hide'}
# Parameter values to simulate:
N <- 200
se1 <- 0.8
sp1 <- 0.95
se2 <- 0.9
sp2 <- 0.99
se3 <- 0.95
sp3 <- 0.95

# Probability of antibody response conditional on disease status (really bad to illustrate the point):
se_antibody <- 0.5
sp_antibody <- 0.75

Populations <- 2
prevalence <- c(0.25,0.75)
Group <- sample(1:Populations, N, replace=TRUE)

# Ensure replicable data:
set.seed(2020-02-18)

# True latent infection status as before:
true <- rbinom(N, 1, prevalence[Group])

# Latent class of antibody response conditional on the true status:
antibody <- rbinom(N, 1, se_antibody*true + (1-sp_antibody)*(1-true))

# Simulate test results for all tests conditional on antibody status:
test1 <- rbinom(N, 1, se1*antibody + (1-sp1)*(1-antibody))
test2 <- rbinom(N, 1, se2*antibody + (1-sp2)*(1-antibody))
test3 <- rbinom(N, 1, se3*antibody + (1-sp3)*(1-antibody))

# Note that the overall sensitivity and specificity of the tests needs to be corrected for the antibody positive step:
overall_se1 <- se_antibody*se1 + (1-se_antibody)*(1-sp1)
overall_sp1 <- sp_antibody*sp1 + (1-sp_antibody)*(1-se1)

overall_se2 <- se_antibody*se2 + (1-se_antibody)*(1-sp2)
overall_sp2 <- sp_antibody*sp2 + (1-sp_antibody)*(1-se2)

overall_se3 <- se_antibody*se3 + (1-se_antibody)*(1-sp3)
overall_sp3 <- sp_antibody*sp3 + (1-sp_antibody)*(1-se3)

simdata <- data.frame(Population=factor(Group), Test1=test1, Test2=test2, Test3=test3)

template_huiwalter(simdata[,c('Population','Test1','Test2','Test3')], outfile='auto3abthw.bug')

results <- run.jags('auto3abthw.bug')

```

Now check the results:

```{r}
results
```

We do a horrible job of estimating prevalence in the second population:

```{r}
prevalence
```

And the test sensitivity/specificity estimates are nowhere near the overall sensitivity/specificity after correcting for antibody status:

```{r}
overall_se1
overall_se2
overall_se3
overall_sp1
overall_sp2
overall_sp3
```

But they are close to the sensitivity/specificity values that are conditional on the antibody status:

```{r}
se1
se2
se3
sp1
sp2
sp3
```

So our model is effectively estimating a latent condition of antibody status, and not a latent condition of true positive status - i.e. the thing that we have pulled out of the hat is not the rabbit that we were hoping for...

`r if(params$presentation) {"\\end{comment}"}`


## What is sensitivity and specificity

TODO:  show how overall Se and Sp relates to probability of test positive given antibody response etc


## Alternative model formulation

TODO: change to explicit serial Disease -> Antibody -> Test etc

Note that autocorrelation is terrible

```{r include=FALSE}
glmhw_definition <- c("model{

  for(i in 1:N){
    Status[i] ~ dcat(prob[i, ])
  
	  prob[i,1] <- (prev[i] * ((1-se[1])*(1-se[2]))) + 
	              ((1-prev[i]) * ((sp[1])*(sp[2])))
	  prob[i,2] <- (prev[i] * ((se[1])*(1-se[2]))) + 
	              ((1-prev[i]) * ((1-sp[1])*(sp[2])))
	  prob[i,3] <- (prev[i] * ((1-se[1])*(se[2]))) + 
	              ((1-prev[i]) * ((sp[1])*(1-sp[2])))
	  prob[i,4] <- (prev[i] * ((se[1])*(se[2]))) + 
	              ((1-prev[i]) * ((1-sp[1])*(1-sp[2])))
	  
	  logit(prev[i]) <- intercept + population_effect[Population[i]]
  }
", "
  intercept ~ dnorm(0, 0.33)
  population_effect[1] <- 0
  for(p in 2:Pops){
    population_effect[p] ~ dnorm(0, 0.1)
  }
  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)T(1-sp[2], )
  sp[2] ~ dbeta(1, 1)

  #data# Status, N, Population, Pops
  #monitor# intercept, population_effect, se, sp
  #inits# intercept, population_effect, se, sp
}
")
cat(glmhw_definition, sep='', file='glm_hw.bug')
```


```{r comment='', echo=FALSE}
cat(glmhw_definition[1], sep='\n')
```

---

```{r comment='', echo=FALSE}
cat(glmhw_definition[2], sep='\n')
```

---

- The main difference is the prior for prevalence in each population

- We also need to give initial values for `intercept` and `population_effect` rather than `prev`, and tell `run.jags` the data frame from which to extract the data (except `N` and `Pops`):

```{r, results='hide', eval=FALSE}
intercept <- list(chain1=-1, chain2=1)
population_effect <- list(chain1=c(NA, 1, -1), chain2=c(NA, -1, 1))
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

simdata$Status <- with(simdata, factor(interaction(Test1, Test2), levels=c('0.0','1.0','0.1','1.1')))
N <- nrow(simdata)
Pops <- length(levels(simdata$Population))
glm_results <- run.jags('glm_hw.bug', n.chains=2, data=simdata)
```


## Publication of your results

STARD-BLCM:  A helpful structure to ensure that papers contain all necessary information

If you use the software, please cite JAGS:

Plummer, M. (2003). JAGS : A Program for Analysis of Bayesian Graphical Models Using Gibbs Sampling JAGS : Just Another Gibbs Sampler. Proceedings of the 3rd International Workshop on Distributed Statistical Computing (DSC 2003), March 20â€“22,Vienna, Austria. ISSN 1609-395X. https://doi.org/10.1.1.13.3406

R:

```{r}
citation()
```

and runjags:

```{r}
citation("runjags")
```


# Discussion session 5

## Points to consider {.fragile}

1. Interpreting the results of latent class models is much more difficult than running them

1. How can we be sure that e.g. `probability of a positive test result conditional on the latent state` is the same thing as `sensitivity`?

1. How can we make sure that our publications contain all of the necessary information to allow others to interpret our findings?

## Exercise

1. Read the STARD-BLCM guidelines, checklist, and examples documents

1. Read the `Diagnosing diagnostic tests` paper

1. Be ready with questions for the group discussion!


## Summary {.fragile}

- Latent class models are MUCH more complex to interpret than traditional models
  - Take time to think about what the latent class means
  
- Think about which tests might be correlated and if you should include covariance terms

- Think about the biology of where your data comes from, particularly if populations are fundamentally different

- Follow the STARD checklist!
